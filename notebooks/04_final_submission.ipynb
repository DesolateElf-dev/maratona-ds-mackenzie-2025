{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Início NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configurações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T01:01:58.400273Z",
     "iopub.status.busy": "2025-12-08T01:01:58.399611Z",
     "iopub.status.idle": "2025-12-08T01:02:07.632515Z",
     "shell.execute_reply": "2025-12-08T01:02:07.630842Z",
     "shell.execute_reply.started": "2025-12-08T01:01:58.400226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MARATONA DE DATA SCIENCE MACKENZIE 2025\n",
    "# Análise Preditiva de Comportamento Bancário\n",
    "# ==============================================================================\n",
    "# \n",
    "# Autor: Eduardo M Sanchez\n",
    "# Data: Dezembro 2025\n",
    "# Objetivo: Prever probabilidade de adesão a depósito bancário\n",
    "# Métrica: ROC AUC\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Configurações\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "\n",
    "print(\"Bibliotecas importadas\")\n",
    "#print(f\"   LightGBM: {lgb.__version__}\")\n",
    "#print(f\"   XGBoost: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T01:13:21.007497Z",
     "iopub.status.busy": "2025-12-08T01:13:21.007156Z",
     "iopub.status.idle": "2025-12-08T01:13:22.883506Z",
     "shell.execute_reply": "2025-12-08T01:13:22.882360Z",
     "shell.execute_reply.started": "2025-12-08T01:13:21.007472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# arquivos\n",
    "TRAIN_DIR = \"../data/raw/analise-preditiva-de-comportamento-bancario/train.csv\"\n",
    "TEST_DIR = \"../data/raw/analise-preditiva-de-comportamento-bancario/test.csv\"\n",
    "SAMPLE_DIR = \"../data/raw/analise-preditiva-de-comportamento-bancario/sample_submission.csv\"\n",
    "\n",
    "# Carregar dados\n",
    "train = pd.read_csv(TRAIN_DIR)\n",
    "test = pd.read_csv(TEST_DIR)\n",
    "sample_submission = pd.read_csv(SAMPLE_DIR)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(\"\\nANÁLISE INICIAL DO DATASET\\n\")\n",
    "\n",
    "# Distribuição do target\n",
    "conversion_rate = train['y'].mean()\n",
    "print(f\"\\nTaxa de conversão: {conversion_rate:.4f} ({conversion_rate*100:.2f}%)\")\n",
    "print(f\"   Não aderiu (0): {(train['y']==0).sum():,} ({(train['y']==0).mean()*100:.1f}%)\")\n",
    "print(f\"   Aderiu (1): {(train['y']==1).sum():,} ({(train['y']==1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Scale pos weight\n",
    "scale_pos_weight = (train['y'] == 0).sum() / (train['y'] == 1).sum()\n",
    "print(f\"\\nScale pos weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelando features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T01:13:31.124867Z",
     "iopub.status.busy": "2025-12-08T01:13:31.124165Z",
     "iopub.status.idle": "2025-12-08T01:13:52.543525Z",
     "shell.execute_reply": "2025-12-08T01:13:52.542414Z",
     "shell.execute_reply.started": "2025-12-08T01:13:31.124837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Iniciando Feature Engineering...\\n\")\n",
    "\n",
    "def create_all_features(df, target=None, train_size=None):\n",
    "    \"\"\"\n",
    "    Cria TODAS as features engineered para máxima performance\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com dados originais\n",
    "        target: Series com target (apenas para train)\n",
    "        train_size: Tamanho do conjunto de treino (para target encoding)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com todas features engineered\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"Criando features binárias...\")\n",
    "    \n",
    "    # ===== FEATURES BINÁRIAS =====\n",
    "    \n",
    "    # Histórico de campanha\n",
    "    df['foi_contatado_antes'] = (df['pdays'] != -1).astype(int)\n",
    "    df['historico_sucesso'] = (df['poutcome'] == 'success').astype(int)\n",
    "    df['historico_failure'] = (df['poutcome'] == 'failure').astype(int)\n",
    "    df['historico_unknown'] = (df['poutcome'] == 'unknown').astype(int)\n",
    "    df['historico_other'] = (df['poutcome'] == 'other').astype(int)\n",
    "    \n",
    "    # Perfil profissional\n",
    "    high_conversion_jobs = ['student', 'retired', 'unemployed', 'management']\n",
    "    low_conversion_jobs = ['blue-collar', 'services', 'housemaid']\n",
    "    df['job_high_conversion'] = df['job'].isin(high_conversion_jobs).astype(int)\n",
    "    df['job_low_conversion'] = df['job'].isin(low_conversion_jobs).astype(int)\n",
    "    df['job_student'] = (df['job'] == 'student').astype(int)\n",
    "    df['job_retired'] = (df['job'] == 'retired').astype(int)\n",
    "    df['job_management'] = (df['job'] == 'management').astype(int)\n",
    "    \n",
    "    # Contato\n",
    "    df['has_effective_contact'] = (df['contact'] != 'unknown').astype(int)\n",
    "    df['contact_cellular'] = (df['contact'] == 'cellular').astype(int)\n",
    "    df['contact_telephone'] = (df['contact'] == 'telephone').astype(int)\n",
    "    df['contact_unknown'] = (df['contact'] == 'unknown').astype(int)\n",
    "    \n",
    "    # Situação financeira\n",
    "    df['has_default'] = (df['default'] == 'yes').astype(int)\n",
    "    df['has_housing'] = (df['housing'] == 'yes').astype(int)\n",
    "    df['has_loan'] = (df['loan'] == 'yes').astype(int)\n",
    "    df['no_debt'] = ((df['default'] == 'no') & (df['loan'] == 'no')).astype(int)\n",
    "    df['full_debt'] = ((df['has_housing'] == 1) & (df['has_loan'] == 1)).astype(int)\n",
    "    \n",
    "    # Status civil\n",
    "    df['is_single'] = (df['marital'] == 'single').astype(int)\n",
    "    df['is_married'] = (df['marital'] == 'married').astype(int)\n",
    "    df['is_divorced'] = (df['marital'] == 'divorced').astype(int)\n",
    "    \n",
    "    # Educação\n",
    "    df['high_education'] = df['education'].isin(['tertiary', 'unknown']).astype(int)\n",
    "    df['education_tertiary'] = (df['education'] == 'tertiary').astype(int)\n",
    "    df['education_secondary'] = (df['education'] == 'secondary').astype(int)\n",
    "    \n",
    "    print(\"Criando features temporais...\")\n",
    "    \n",
    "    # ===== FEATURES TEMPORAIS =====\n",
    "    \n",
    "    # Sazonalidade\n",
    "    high_season_months = ['mar', 'sep', 'oct', 'dec']\n",
    "    low_season_months = ['may', 'jun', 'jul']\n",
    "    df['is_high_season'] = df['month'].isin(high_season_months).astype(int)\n",
    "    df['is_low_season'] = df['month'].isin(low_season_months).astype(int)\n",
    "    df['is_may'] = (df['month'] == 'may').astype(int)\n",
    "    df['is_mar'] = (df['month'] == 'mar').astype(int)\n",
    "    df['is_dec'] = (df['month'] == 'dec').astype(int)\n",
    "    \n",
    "    # Trimestre\n",
    "    month_to_quarter = {\n",
    "        'jan': 1, 'feb': 1, 'mar': 1,\n",
    "        'apr': 2, 'may': 2, 'jun': 2,\n",
    "        'jul': 3, 'aug': 3, 'sep': 3,\n",
    "        'oct': 4, 'nov': 4, 'dec': 4\n",
    "    }\n",
    "    df['quarter'] = df['month'].map(month_to_quarter)\n",
    "    df['quarter_1'] = (df['quarter'] == 1).astype(int)\n",
    "    df['quarter_4'] = (df['quarter'] == 4).astype(int)\n",
    "    \n",
    "    # Período do mês\n",
    "    df['day_period'] = pd.cut(df['day'], bins=[0, 10, 20, 31], labels=[0, 1, 2]).astype(int)\n",
    "    df['inicio_mes'] = (df['day'] <= 5).astype(int)\n",
    "    df['meio_mes'] = ((df['day'] > 10) & (df['day'] <= 20)).astype(int)\n",
    "    df['fim_mes'] = (df['day'] >= 25).astype(int)\n",
    "    df['day_squared'] = df['day'] ** 2\n",
    "    \n",
    "    print(\"Criando features de campanha...\")\n",
    "    \n",
    "    # ===== FEATURES DE CAMPANHA =====\n",
    "    \n",
    "    df['total_contacts'] = df['campaign'] + df['previous']\n",
    "    df['contact_frequency'] = df['campaign'] / (df['campaign'].max() + 1)\n",
    "    df['has_previous'] = (df['previous'] > 0).astype(int)\n",
    "    df['multiple_previous'] = (df['previous'] > 1).astype(int)\n",
    "    \n",
    "    # Perfis de cliente\n",
    "    df['cliente_frio'] = ((df['campaign'] > 3) & (df['poutcome'] != 'success')).astype(int)\n",
    "    df['cliente_quente'] = ((df['campaign'] <= 2) & (df['poutcome'] == 'success')).astype(int)\n",
    "    df['cliente_novo'] = ((df['previous'] == 0) & (df['poutcome'] == 'unknown')).astype(int)\n",
    "    df['cliente_insistente'] = (df['campaign'] > 5).astype(int)\n",
    "    \n",
    "    # Categorias de pdays\n",
    "    df['pdays_category'] = pd.cut(df['pdays'], \n",
    "                                    bins=[-2, -1, 30, 90, 180, 999],\n",
    "                                    labels=[0, 1, 2, 3, 4]).astype(int)\n",
    "    df['never_contacted'] = (df['pdays'] == -1).astype(int)\n",
    "    df['contato_recente'] = ((df['pdays'] > 0) & (df['pdays'] <= 30)).astype(int)\n",
    "    df['contato_medio'] = ((df['pdays'] > 30) & (df['pdays'] <= 180)).astype(int)\n",
    "    df['contato_antigo'] = (df['pdays'] > 180).astype(int)\n",
    "    \n",
    "    # Intensidade\n",
    "    df['high_campaign_intensity'] = (df['campaign'] > df['campaign'].median()).astype(int)\n",
    "    df['low_campaign_intensity'] = (df['campaign'] == 1).astype(int)\n",
    "    \n",
    "    # Campaign buckets\n",
    "    df['campaign_bucket'] = pd.cut(df['campaign'], \n",
    "                                     bins=[0, 1, 2, 3, 5, 100],\n",
    "                                     labels=[0, 1, 2, 3, 4]).astype(int)\n",
    "    \n",
    "    print(\"Criando features financeiras...\")\n",
    "    \n",
    "    # ===== FEATURES FINANCEIRAS =====\n",
    "    \n",
    "    # Categorização de balance\n",
    "    df['balance_category'] = pd.cut(df['balance'], \n",
    "                                     bins=[-np.inf, 0, 500, 1000, 3000, 5000, np.inf],\n",
    "                                     labels=[0, 1, 2, 3, 4, 5]).astype(int)\n",
    "    \n",
    "    df['balance_negative'] = (df['balance'] < 0).astype(int)\n",
    "    df['balance_very_low'] = (df['balance'] < 500).astype(int)\n",
    "    df['balance_low'] = ((df['balance'] >= 500) & (df['balance'] < 1000)).astype(int)\n",
    "    df['balance_medium'] = ((df['balance'] >= 1000) & (df['balance'] < 5000)).astype(int)\n",
    "    df['balance_high'] = (df['balance'] >= 5000).astype(int)\n",
    "    df['balance_very_high'] = (df['balance'] > 10000).astype(int)\n",
    "    df['balance_zero'] = (df['balance'] == 0).astype(int)\n",
    "    \n",
    "    # Transformações\n",
    "    df['balance_per_age'] = df['balance'] / (df['age'] + 1)\n",
    "    df['balance_log'] = np.log1p(df['balance'] + abs(df['balance'].min()) + 1)\n",
    "    df['balance_sqrt'] = np.sqrt(df['balance'] + abs(df['balance'].min()))\n",
    "    df['balance_abs'] = np.abs(df['balance'])\n",
    "    \n",
    "    # Dívidas\n",
    "    df['total_loans'] = df['has_housing'] + df['has_loan'] + df['has_default']\n",
    "    df['sem_dividas'] = (df['total_loans'] == 0).astype(int)\n",
    "    df['uma_divida'] = (df['total_loans'] == 1).astype(int)\n",
    "    df['multiplas_dividas'] = (df['total_loans'] >= 2).astype(int)\n",
    "    \n",
    "    # Perfis\n",
    "    df['perfil_premium'] = ((df['balance'] > df['balance'].median()) & \n",
    "                            (df['sem_dividas'] == 1)).astype(int)\n",
    "    df['perfil_risco'] = ((df['balance'] < 0) | (df['has_default'] == 1)).astype(int)\n",
    "    \n",
    "    print(\"Criando features demográficas...\")\n",
    "    \n",
    "    # ===== FEATURES DEMOGRÁFICAS =====\n",
    "    \n",
    "    # Faixas etárias\n",
    "    df['age_group'] = pd.cut(df['age'], \n",
    "                              bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "                              labels=[0, 1, 2, 3, 4, 5]).astype(int)\n",
    "    \n",
    "    df['is_very_young'] = (df['age'] < 25).astype(int)\n",
    "    df['is_young'] = ((df['age'] >= 25) & (df['age'] < 35)).astype(int)\n",
    "    df['is_middle_age'] = ((df['age'] >= 35) & (df['age'] < 55)).astype(int)\n",
    "    df['is_senior'] = (df['age'] >= 60).astype(int)\n",
    "    df['is_elderly'] = (df['age'] >= 70).astype(int)\n",
    "    df['working_age'] = ((df['age'] >= 25) & (df['age'] < 65)).astype(int)\n",
    "    \n",
    "    # Transformações de idade\n",
    "    df['age_squared'] = df['age'] ** 2\n",
    "    df['age_log'] = np.log1p(df['age'])\n",
    "    \n",
    "    # Perfis combinados\n",
    "    df['estudante_jovem'] = ((df['job'] == 'student') & (df['age'] < 30)).astype(int)\n",
    "    df['aposentado_senior'] = ((df['job'] == 'retired') & (df['age'] >= 60)).astype(int)\n",
    "    df['jovem_solteiro'] = ((df['age'] < 35) & (df['marital'] == 'single')).astype(int)\n",
    "    df['adulto_casado'] = ((df['age'] >= 35) & (df['marital'] == 'married')).astype(int)\n",
    "    \n",
    "    # Status socioeconômico\n",
    "    high_status_jobs = ['management', 'self-employed', 'entrepreneur']\n",
    "    df['high_status'] = ((df['job'].isin(high_status_jobs)) & \n",
    "                         (df['education'] == 'tertiary')).astype(int)\n",
    "    df['poupador_potencial'] = ((df['balance'] > df['balance'].median()) & \n",
    "                                 (df['working_age'] == 1) &\n",
    "                                 (df['sem_dividas'] == 1)).astype(int)\n",
    "    \n",
    "    print(\"Criando features de interação...\")\n",
    "    \n",
    "    # ===== FEATURES DE INTERAÇÃO =====\n",
    "    \n",
    "    # Interações numéricas\n",
    "    df['age_balance_ratio'] = (df['age'] * df['balance']) / 1000\n",
    "    df['age_balance_product'] = df['age'] * df['balance'] / 10000\n",
    "    df['age_campaign'] = df['age'] * df['campaign']\n",
    "    df['age_duration'] = df['age'] * df['duration'] / 100\n",
    "    df['balance_duration'] = df['balance'] * df['duration'] / 1000\n",
    "    df['balance_campaign'] = df['balance'] * df['campaign']\n",
    "    \n",
    "    # Interações com histórico\n",
    "    df['campaign_success_interaction'] = df['campaign'] * df['historico_sucesso']\n",
    "    df['previous_success_interaction'] = df['previous'] * df['historico_sucesso']\n",
    "    df['duration_success_interaction'] = df['duration'] * df['historico_sucesso']\n",
    "    \n",
    "    # Interações categóricas\n",
    "    df['education_job_match'] = ((df['education'] == 'tertiary') & \n",
    "                                  (df['job'].isin(['management', 'technician', 'services']))).astype(int)\n",
    "    df['contact_timing'] = df['contact_cellular'] * df['is_high_season']\n",
    "    df['contact_profile'] = df['contact_cellular'] * df['job_high_conversion']\n",
    "    \n",
    "    # Razões\n",
    "    df['balance_loan_ratio'] = df['balance'] / (df['total_loans'] + 1)\n",
    "    df['previous_campaign_ratio'] = df['previous'] / (df['campaign'] + 1)\n",
    "    df['duration_campaign_ratio'] = df['duration'] / (df['campaign'] + 1)\n",
    "    \n",
    "    # Peso do histórico\n",
    "    poutcome_weight = {'success': 3, 'failure': -1, 'other': 0, 'unknown': 0}\n",
    "    df['poutcome_numeric'] = df['poutcome'].map(poutcome_weight)\n",
    "    df['weighted_history'] = df['previous'] * df['poutcome_numeric']\n",
    "    df['weighted_contacts'] = df['total_contacts'] * df['poutcome_numeric']\n",
    "    \n",
    "    # Score composto\n",
    "    df['conversion_score'] = (\n",
    "        df['historico_sucesso'] * 3 +\n",
    "        df['job_high_conversion'] * 2 +\n",
    "        df['is_high_season'] * 2 +\n",
    "        df['contact_cellular'] * 1 +\n",
    "        df['high_education'] * 1 -\n",
    "        df['cliente_frio'] * 2\n",
    "    )\n",
    "    \n",
    "    print(\"Aplicando encoding...\")\n",
    "    \n",
    "    # ===== ENCODING DE VARIÁVEIS CATEGÓRICAS =====\n",
    "    \n",
    "    # Month (ordinal)\n",
    "    month_order = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "                   'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
    "    df['month_encoded'] = df['month'].map(month_order)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month_encoded'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month_encoded'] / 12)\n",
    "    \n",
    "    # Target Encoding para job e education\n",
    "    if target is not None and train_size is not None:\n",
    "        df_train = df.iloc[:train_size].copy()\n",
    "        df_train['target'] = target\n",
    "        \n",
    "        # Job\n",
    "        job_means = df_train.groupby('job')['target'].mean()\n",
    "        job_counts = df_train.groupby('job').size()\n",
    "        global_mean = target.mean()\n",
    "        \n",
    "        # Smoothing (para evitar overfitting)\n",
    "        smoothing = 10\n",
    "        df['job_encoded'] = df['job'].map(\n",
    "            lambda x: (job_means.get(x, global_mean) * job_counts.get(x, 0) + \n",
    "                      global_mean * smoothing) / (job_counts.get(x, 0) + smoothing)\n",
    "        )\n",
    "        \n",
    "        # Education\n",
    "        education_means = df_train.groupby('education')['target'].mean()\n",
    "        education_counts = df_train.groupby('education').size()\n",
    "        df['education_encoded'] = df['education'].map(\n",
    "            lambda x: (education_means.get(x, global_mean) * education_counts.get(x, 0) + \n",
    "                      global_mean * smoothing) / (education_counts.get(x, 0) + smoothing)\n",
    "        )\n",
    "    else:\n",
    "        le_job = LabelEncoder()\n",
    "        le_education = LabelEncoder()\n",
    "        df['job_encoded'] = le_job.fit_transform(df['job'])\n",
    "        df['education_encoded'] = le_education.fit_transform(df['education'])\n",
    "    \n",
    "    # Frequency encoding\n",
    "    for col in ['job', 'education', 'marital']:\n",
    "        freq = df[col].value_counts(normalize=True)\n",
    "        df[f'{col}_freq'] = df[col].map(freq)\n",
    "    \n",
    "    print(\"Finalizando...\")\n",
    "    \n",
    "    # ===== REMOVER FEATURES ORIGINAIS CATEGÓRICAS =====\n",
    "    \n",
    "    features_to_drop = ['job', 'marital', 'education', 'default', 'housing', \n",
    "                        'loan', 'contact', 'month', 'poutcome', 'poutcome_numeric']\n",
    "    df = df.drop(columns=features_to_drop, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar feature engineering\n",
    "print(\"Processando train e test...\")\n",
    "\n",
    "df_combined = pd.concat([train.drop('y', axis=1), test], axis=0, ignore_index=True)\n",
    "df_processed = create_all_features(df_combined, target=train['y'], train_size=len(train))\n",
    "\n",
    "# Separar\n",
    "X_train = df_processed.iloc[:len(train)].copy()\n",
    "X_test = df_processed.iloc[len(train):].copy()\n",
    "y_train = train['y']\n",
    "\n",
    "# Remover id\n",
    "X_train = X_train.drop('id', axis=1, errors='ignore')\n",
    "X_test = X_test.drop('id', axis=1, errors='ignore')\n",
    "'''\n",
    "print(f\"\\nFeature Engineering concluído\")\n",
    "print(f\"   Train shape: {X_train.shape}\")\n",
    "print(f\"   Test shape: {X_test.shape}\")\n",
    "print(f\"   Total de features criadas: {X_train.shape[1]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ENSEMBLE DE MODELOS - LIGHTGBM + XGBOOST + CATBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T01:13:55.350851Z",
     "iopub.status.busy": "2025-12-08T01:13:55.350552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"TREINAMENTO DO MODELO - LIGHTGBM OTIMIZADO\")\n",
    "\n",
    "# Configuração da validação cruzada\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# ===== HIPERPARÂMETROS OTIMIZADOS =====\n",
    "\n",
    "# LightGBM\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 88,\n",
    "    'learning_rate': 0.07161197523282926,\n",
    "    'feature_fraction': 0.8900436534571514,\n",
    "    'bagging_fraction': 0.7870692246364229,\n",
    "    'bagging_freq': 1,\n",
    "    'min_child_samples': 43,\n",
    "    'max_depth': 12,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.85,\n",
    "    'colsample_bytree': 0.85,\n",
    "    'min_child_weight': 10,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "# CatBoost\n",
    "cat_params = {\n",
    "    'iterations': 1500,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'AUC',\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'verbose': 0,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "# Armazenamento de modelos e predições\n",
    "lgb_models = []\n",
    "xgb_models = []\n",
    "cat_models = []\n",
    "calibrators_lgb = []\n",
    "calibrators_xgb = []\n",
    "calibrators_cat = []\n",
    "\n",
    "lgb_test_preds = []\n",
    "xgb_test_preds = []\n",
    "cat_test_preds = []\n",
    "\n",
    "lgb_cv_scores = []\n",
    "xgb_cv_scores = []\n",
    "cat_cv_scores = []\n",
    "\n",
    "oof_lgb = np.zeros(len(X_train))\n",
    "oof_xgb = np.zeros(len(X_train))\n",
    "oof_cat = np.zeros(len(X_train))\n",
    "\n",
    "print(\"Iniciando treinamento do ensemble...\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FOLD {fold}/{N_FOLDS}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Split\n",
    "    X_train_fold = X_train.iloc[train_idx]\n",
    "    X_val_fold = X_train.iloc[val_idx]\n",
    "    y_train_fold = y_train.iloc[train_idx]\n",
    "    y_val_fold = y_train.iloc[val_idx]\n",
    "    \n",
    "    # ===== LIGHTGBM =====\n",
    "    print(\"Treinando LightGBM...\")\n",
    "    train_data_lgb = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data_lgb = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data_lgb)\n",
    "    \n",
    "    model_lgb = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data_lgb,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data_lgb],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(100, verbose=False),\n",
    "            lgb.log_evaluation(200)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_val_pred_lgb = model_lgb.predict(X_val_fold, num_iteration=model_lgb.best_iteration)\n",
    "    oof_lgb[val_idx] = y_val_pred_lgb\n",
    "    auc_lgb = roc_auc_score(y_val_fold, y_val_pred_lgb)\n",
    "    lgb_cv_scores.append(auc_lgb)\n",
    "    \n",
    "    # Calibrar LightGBM\n",
    "    cal_lgb = IsotonicRegression(out_of_bounds='clip')\n",
    "    cal_lgb.fit(y_val_pred_lgb, y_val_fold)\n",
    "    calibrators_lgb.append(cal_lgb)\n",
    "    \n",
    "    # Predição no test\n",
    "    y_test_pred_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n",
    "    lgb_test_preds.append(y_test_pred_lgb)\n",
    "    lgb_models.append(model_lgb)\n",
    "    \n",
    "    print(f\"   LightGBM AUC: {auc_lgb:.5f} | Best iteration: {model_lgb.best_iteration}\")\n",
    "    \n",
    "    # ===== XGBOOST =====\n",
    "    print(\"\\nTreinando XGBoost...\")\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n",
    "    dval = xgb.DMatrix(X_val_fold, label=y_val_fold)\n",
    "    \n",
    "    model_xgb = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dval, 'valid')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=200\n",
    "    )\n",
    "    \n",
    "    y_val_pred_xgb = model_xgb.predict(dval, iteration_range=(0, model_xgb.best_iteration))\n",
    "    oof_xgb[val_idx] = y_val_pred_xgb\n",
    "    auc_xgb = roc_auc_score(y_val_fold, y_val_pred_xgb)\n",
    "    xgb_cv_scores.append(auc_xgb)\n",
    "    \n",
    "    # Calibrar XGBoost\n",
    "    cal_xgb = IsotonicRegression(out_of_bounds='clip')\n",
    "    cal_xgb.fit(y_val_pred_xgb, y_val_fold)\n",
    "    calibrators_xgb.append(cal_xgb)\n",
    "    \n",
    "    # Predição no test\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_test_pred_xgb = model_xgb.predict(dtest, iteration_range=(0, model_xgb.best_iteration))\n",
    "    xgb_test_preds.append(y_test_pred_xgb)\n",
    "    xgb_models.append(model_xgb)\n",
    "    \n",
    "    print(f\"   XGBoost AUC: {auc_xgb:.5f} | Best iteration: {model_xgb.best_iteration}\")\n",
    "    \n",
    "    # ===== CATBOOST =====\n",
    "    print(\"\\nTreinando CatBoost...\")\n",
    "    model_cat = CatBoostClassifier(**cat_params)\n",
    "    \n",
    "    model_cat.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_val_fold, y_val_fold),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_val_pred_cat = model_cat.predict_proba(X_val_fold)[:, 1]\n",
    "    oof_cat[val_idx] = y_val_pred_cat\n",
    "    auc_cat = roc_auc_score(y_val_fold, y_val_pred_cat)\n",
    "    cat_cv_scores.append(auc_cat)\n",
    "    \n",
    "    # Calibrar CatBoost\n",
    "    cal_cat = IsotonicRegression(out_of_bounds='clip')\n",
    "    cal_cat.fit(y_val_pred_cat, y_val_fold)\n",
    "    calibrators_cat.append(cal_cat)\n",
    "    \n",
    "    # Predição no test\n",
    "    y_test_pred_cat = model_cat.predict_proba(X_test)[:, 1]\n",
    "    cat_test_preds.append(y_test_pred_cat)\n",
    "    cat_models.append(model_cat)\n",
    "    \n",
    "    print(f\"   CatBoost AUC: {auc_cat:.5f}\")\n",
    "    \n",
    "    # Resumo do fold\n",
    "    print(f\"\\nResumo do Fold {fold}:\")\n",
    "    print(f\"   LightGBM: {auc_lgb:.5f}\")\n",
    "    print(f\"   XGBoost:  {auc_xgb:.5f}\")\n",
    "    print(f\"   CatBoost: {auc_cat:.5f}\")\n",
    "\n",
    "# ===== RESULTADOS FINAIS DA CV =====\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RESULTADOS FINAIS DO ENSEMBLE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Out-of-fold AUCs\n",
    "oof_auc_lgb = roc_auc_score(y_train, oof_lgb)\n",
    "oof_auc_xgb = roc_auc_score(y_train, oof_xgb)\n",
    "oof_auc_cat = roc_auc_score(y_train, oof_cat)\n",
    "\n",
    "print(\"AUC-ROC por Modelo:\\n\")\n",
    "print(f\"{'Modelo':<15} {'Mean AUC':<12} {'Std AUC':<12} {'OOF AUC':<12}\")\n",
    "print(\"-\" * 51)\n",
    "print(f\"{'LightGBM':<15} {np.mean(lgb_cv_scores):<12.5f} {np.std(lgb_cv_scores):<12.5f} {oof_auc_lgb:<12.5f}\")\n",
    "print(f\"{'XGBoost':<15} {np.mean(xgb_cv_scores):<12.5f} {np.std(xgb_cv_scores):<12.5f} {oof_auc_xgb:<12.5f}\")\n",
    "print(f\"{'CatBoost':<15} {np.mean(cat_cv_scores):<12.5f} {np.std(cat_cv_scores):<12.5f} {oof_auc_cat:<12.5f}\")\n",
    "\n",
    "# Ensemble de OOF\n",
    "oof_ensemble = (oof_lgb + oof_xgb + oof_cat) / 3\n",
    "oof_auc_ensemble = roc_auc_score(y_train, oof_ensemble)\n",
    "print(f\"\\nENSEMBLE OOF AUC: {oof_auc_ensemble:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibrando e gerando submissões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T16:12:02.817682Z",
     "iopub.status.busy": "2025-12-07T16:12:02.817427Z",
     "iopub.status.idle": "2025-12-07T16:12:02.952319Z",
     "shell.execute_reply": "2025-12-07T16:12:02.951486Z",
     "shell.execute_reply.started": "2025-12-07T16:12:02.817660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"CALIBRAÇÃO E PREDIÇÕES FINAIS\")\n",
    "\n",
    "# Média das predições não calibradas\n",
    "lgb_test_mean = np.mean(lgb_test_preds, axis=0)\n",
    "xgb_test_mean = np.mean(xgb_test_preds, axis=0)\n",
    "cat_test_mean = np.mean(cat_test_preds, axis=0)\n",
    "\n",
    "# Aplicar calibração\n",
    "lgb_test_calibrated = []\n",
    "xgb_test_calibrated = []\n",
    "cat_test_calibrated = []\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    lgb_test_calibrated.append(calibrators_lgb[i].predict(lgb_test_preds[i]))\n",
    "    xgb_test_calibrated.append(calibrators_xgb[i].predict(xgb_test_preds[i]))\n",
    "    cat_test_calibrated.append(calibrators_cat[i].predict(cat_test_preds[i]))\n",
    "\n",
    "lgb_test_cal_mean = np.mean(lgb_test_calibrated, axis=0)\n",
    "xgb_test_cal_mean = np.mean(xgb_test_calibrated, axis=0)\n",
    "cat_test_cal_mean = np.mean(cat_test_calibrated, axis=0)\n",
    "\n",
    "# ===== ESTRATÉGIAS DE ENSEMBLE =====\n",
    "\n",
    "print(\"Criando diferentes estratégias de ensemble...\\n\")\n",
    "\n",
    "# 1. Média simples (calibrado)\n",
    "ensemble_mean = (lgb_test_cal_mean + xgb_test_cal_mean + cat_test_cal_mean) / 3\n",
    "\n",
    "# 2. Média ponderada por performance (calibrado)\n",
    "total_score = oof_auc_lgb + oof_auc_xgb + oof_auc_cat\n",
    "w_lgb = oof_auc_lgb / total_score\n",
    "w_xgb = oof_auc_xgb / total_score\n",
    "w_cat = oof_auc_cat / total_score\n",
    "\n",
    "ensemble_weighted = (w_lgb * lgb_test_cal_mean + \n",
    "                     w_xgb * xgb_test_cal_mean + \n",
    "                     w_cat * cat_test_cal_mean)\n",
    "\n",
    "# 3. Rank averaging (mais robusto a outliers)\n",
    "from scipy.stats import rankdata\n",
    "lgb_ranks = rankdata(lgb_test_cal_mean) / len(lgb_test_cal_mean)\n",
    "xgb_ranks = rankdata(xgb_test_cal_mean) / len(xgb_test_cal_mean)\n",
    "cat_ranks = rankdata(cat_test_cal_mean) / len(cat_test_cal_mean)\n",
    "ensemble_rank = (lgb_ranks + xgb_ranks + cat_ranks) / 3\n",
    "\n",
    "# 4. Melhor modelo individual (LightGBM se for o melhor)\n",
    "best_model_pred = lgb_test_cal_mean if oof_auc_lgb >= max(oof_auc_xgb, oof_auc_cat) else (\n",
    "    xgb_test_cal_mean if oof_auc_xgb >= oof_auc_cat else cat_test_cal_mean\n",
    ")\n",
    "\n",
    "# Estatísticas\n",
    "print(\"ESTATÍSTICAS DAS PREDIÇÕES:\\n\")\n",
    "print(f\"{'Estratégia':<30} {'Média':<12} {'Mediana':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "strategies = {\n",
    "    'LightGBM (calibrado)': lgb_test_cal_mean,\n",
    "    'XGBoost (calibrado)': xgb_test_cal_mean,\n",
    "    'CatBoost (calibrado)': cat_test_cal_mean,\n",
    "    'Ensemble Média Simples': ensemble_mean,\n",
    "    'Ensemble Média Ponderada': ensemble_weighted,\n",
    "    'Ensemble Rank Averaging': ensemble_rank\n",
    "}\n",
    "\n",
    "for name, pred in strategies.items():\n",
    "    print(f\"{name:<30} {pred.mean():<12.5f} {np.median(pred):<12.5f} {pred.min():<12.5f} {pred.max():<12.5f}\")\n",
    "\n",
    "print(f\"\\nTaxa de conversão esperada no train: {y_train.mean():.5f} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"\\nPesos do ensemble ponderado:\")\n",
    "print(f\"   LightGBM: {w_lgb:.4f} ({w_lgb*100:.2f}%)\")\n",
    "print(f\"   XGBoost:  {w_xgb:.4f} ({w_xgb*100:.2f}%)\")\n",
    "print(f\"   CatBoost: {w_cat:.4f} ({w_cat*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T16:12:02.953476Z",
     "iopub.status.busy": "2025-12-07T16:12:02.953208Z",
     "iopub.status.idle": "2025-12-07T16:12:03.423409Z",
     "shell.execute_reply": "2025-12-07T16:12:03.422443Z",
     "shell.execute_reply.started": "2025-12-07T16:12:02.953448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"ANÁLISE DE FEATURE IMPORTANCE\")\n",
    "\n",
    "# LightGBM importance\n",
    "lgb_importances = []\n",
    "for model in lgb_models:\n",
    "    lgb_importances.append(model.feature_importance(importance_type='gain'))\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance_lgb': np.mean(lgb_importances, axis=0)\n",
    "})\n",
    "\n",
    "# XGBoost importance\n",
    "xgb_importances = []\n",
    "for model in xgb_models:\n",
    "    importance_dict = model.get_score(importance_type='gain')\n",
    "    importance_array = np.zeros(len(X_train.columns))\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        importance_array[i] = importance_dict.get(f'f{i}', 0)\n",
    "    xgb_importances.append(importance_array)\n",
    "\n",
    "feature_importance['importance_xgb'] = np.mean(xgb_importances, axis=0)\n",
    "\n",
    "# CatBoost importance\n",
    "cat_importances = []\n",
    "for model in cat_models:\n",
    "    cat_importances.append(model.get_feature_importance())\n",
    "\n",
    "feature_importance['importance_cat'] = np.mean(cat_importances, axis=0)\n",
    "\n",
    "# Importância média normalizada\n",
    "feature_importance['importance_mean'] = (\n",
    "    feature_importance[['importance_lgb', 'importance_xgb', 'importance_cat']]\n",
    "    .apply(lambda x: x / x.sum(), axis=0)\n",
    "    .mean(axis=1)\n",
    ")\n",
    "\n",
    "feature_importance = feature_importance.sort_values('importance_mean', ascending=False)\n",
    "\n",
    "# Top 20\n",
    "print(\"Top 20 Features Mais Importantes (média dos 3 modelos):\\n\")\n",
    "for idx, row in feature_importance.head(20).iterrows():\n",
    "    print(f\"   {row['feature']:<35} {row['importance_mean']:>8.5f}\")\n",
    "\n",
    "# Visualização\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (col, title) in enumerate([\n",
    "    ('importance_lgb', 'LightGBM'),\n",
    "    ('importance_xgb', 'XGBoost'),\n",
    "    ('importance_cat', 'CatBoost')\n",
    "]):\n",
    "    top20 = feature_importance.nlargest(20, col)\n",
    "    axes[idx].barh(top20['feature'][::-1], top20[col][::-1], color='steelblue')\n",
    "    axes[idx].set_xlabel('Importance', fontweight='bold')\n",
    "    axes[idx].set_title(f'Top 20 Features - {title}', fontweight='bold')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Criando Submissão final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T16:12:03.424711Z",
     "iopub.status.busy": "2025-12-07T16:12:03.424295Z",
     "iopub.status.idle": "2025-12-07T16:12:04.042364Z",
     "shell.execute_reply": "2025-12-07T16:12:04.041375Z",
     "shell.execute_reply.started": "2025-12-07T16:12:03.424686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"GERANDO ARQUIVOS DE SUBMISSÃO\")\n",
    "\n",
    "# Criar submissões para cada estratégia\n",
    "submissions = {\n",
    "    'submission_lgb_calibrated.csv': lgb_test_cal_mean,\n",
    "    'submission_xgb_calibrated.csv': xgb_test_cal_mean,\n",
    "    'submission_cat_calibrated.csv': cat_test_cal_mean,\n",
    "    'submission_ensemble_mean.csv': ensemble_mean,\n",
    "    'submission_ensemble_weighted.csv': ensemble_weighted,\n",
    "    'submission_ensemble_rank.csv': ensemble_rank,\n",
    "    'submission.csv': ensemble_weighted  # Principal\n",
    "}\n",
    "\n",
    "for filename, predictions in submissions.items():\n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample_submission['id'],\n",
    "        'y': predictions\n",
    "    })\n",
    "    submission.to_csv(f\"../submissions/{filename}\", index=False)\n",
    "    print(f\"OK {filename}\")\n",
    "\n",
    "print(f\"\\nSUBMISSÃO PRINCIPAL: submission.csv (Ensemble Ponderado)\")\n",
    "print(f\"   Média: {ensemble_weighted.mean():.5f} ({ensemble_weighted.mean()*100:.2f}%)\")\n",
    "\n",
    "# Mostrar primeiras linhas\n",
    "print(f\"\\nPrimeiras linhas da submissão principal:\")\n",
    "display(pd.DataFrame({\n",
    "    'id': sample_submission['id'],\n",
    "    'y': ensemble_weighted\n",
    "}).head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14536558,
     "sourceId": 121638,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
