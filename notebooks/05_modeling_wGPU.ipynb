{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f011b0b0",
   "metadata": {},
   "source": [
    "# Setup inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ed059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTEBOOK: 05_modeling_wGPU.ipynb\n",
    "# Modelagem com aceleração GPU e otimização de hiperparâmetros\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP INICIAL\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuracoes\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Carregar dados processados\n",
    "train = pd.read_csv('../data/processed/train_processed.csv')\n",
    "test = pd.read_csv('../data/processed/test_processed.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Separar features e target\n",
    "y = train['y']\n",
    "X = train.drop(['y', 'id'], axis=1, errors='ignore')\n",
    "X_test = test.drop('id', axis=1, errors='ignore')\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Verificar desbalanceamento\n",
    "print(\"\\nDistribuicao do target:\")\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# Calcular scale_pos_weight para modelos tree-based\n",
    "scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"\\nScale pos weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1910b8",
   "metadata": {},
   "source": [
    "# Análise básica dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10946004",
   "metadata": {},
   "source": [
    "## Stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURACAO DA VALIDACAO CRUZADA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Stratified K-Fold para manter proporcao das classes\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "def evaluate_model_cv(model, X, y, cv=skf):\n",
    "    \"\"\"\n",
    "    Avalia modelo usando cross-validation\n",
    "    Retorna media e desvio padrao do AUC-ROC\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Treinar\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Prever probabilidades\n",
    "        y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        # Calcular AUC\n",
    "        auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "        scores.append(auc)\n",
    "        \n",
    "        print(f\"Fold {fold} AUC: {auc:.5f}\")\n",
    "    \n",
    "    mean_auc = np.mean(scores)\n",
    "    std_auc = np.std(scores)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"Mean AUC: {mean_auc:.5f} (+/- {std_auc:.5f})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return mean_auc, std_auc, scores\n",
    "\n",
    "print(\"Validacao cruzada configurada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf282b3",
   "metadata": {},
   "source": [
    "## Baseline - Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058927eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO BASELINE: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Modelo com class_weight para lidar com desbalanceamento\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "# Avaliar com CV\n",
    "lr_mean_auc, lr_std_auc, lr_scores = evaluate_model_cv(lr_model, X, y)\n",
    "\n",
    "# Salvar resultado\n",
    "baseline_results = {\n",
    "    'model': 'Logistic Regression',\n",
    "    'mean_auc': lr_mean_auc,\n",
    "    'std_auc': lr_std_auc,\n",
    "    'scores': lr_scores\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1039c9",
   "metadata": {},
   "source": [
    "## LighGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO: LIGHTGBM (GPU)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 20,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_scores = []\n",
    "lgb_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\nTraining Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Criar datasets LightGBM\n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    # Treinar\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Prever\n",
    "    y_pred_proba = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "    \n",
    "    lgb_scores.append(auc)\n",
    "    lgb_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}\")\n",
    "\n",
    "lgb_mean_auc = np.mean(lgb_scores)\n",
    "lgb_std_auc = np.std(lgb_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"LightGBM (GPU) Mean AUC: {lgb_mean_auc:.5f} (+/- {lgb_std_auc:.5f})\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "lgb_results = {\n",
    "    'model': 'LightGBM (GPU)',\n",
    "    'mean_auc': lgb_mean_auc,\n",
    "    'std_auc': lgb_std_auc,\n",
    "    'scores': lgb_scores,\n",
    "    'models': lgb_models\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1cb960",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcaf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO: XGBOOST (GPU)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 1000,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "xgb_scores = []\n",
    "xgb_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\nTraining Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    \n",
    "    # Treinar\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_val_fold, y_val_fold)],\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Prever\n",
    "    y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "    auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "    \n",
    "    xgb_scores.append(auc)\n",
    "    xgb_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}\")\n",
    "\n",
    "xgb_mean_auc = np.mean(xgb_scores)\n",
    "xgb_std_auc = np.std(xgb_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"XGBoost (GPU) Mean AUC: {xgb_mean_auc:.5f} (+/- {xgb_std_auc:.5f})\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "xgb_results = {\n",
    "    'model': 'XGBoost (GPU)',\n",
    "    'mean_auc': xgb_mean_auc,\n",
    "    'std_auc': xgb_std_auc,\n",
    "    'scores': xgb_scores,\n",
    "    'models': xgb_models\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28e8ab",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e619f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO: CATBOOST (GPU)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "cat_params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'Logloss',\n",
    "    'task_type': 'GPU',\n",
    "    'devices': '0',\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'verbose': 100,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "cat_scores = []\n",
    "cat_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\nTraining Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(**cat_params)\n",
    "    \n",
    "    # Treinar\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_val_fold, y_val_fold),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Prever e calcular AUC manualmente\n",
    "    y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "    auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "    \n",
    "    cat_scores.append(auc)\n",
    "    cat_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}\")\n",
    "\n",
    "cat_mean_auc = np.mean(cat_scores)\n",
    "cat_std_auc = np.std(cat_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CatBoost (GPU) Mean AUC: {cat_mean_auc:.5f} (+/- {cat_std_auc:.5f})\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "cat_results = {\n",
    "    'model': 'CatBoost (GPU)',\n",
    "    'mean_auc': cat_mean_auc,\n",
    "    'std_auc': cat_std_auc,\n",
    "    'scores': cat_scores,\n",
    "    'models': cat_models\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d2a09",
   "metadata": {},
   "source": [
    "# Análise resultados parciais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5bbea6",
   "metadata": {},
   "source": [
    "## Análise de feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALISE DE FEATURE IMPORTANCE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Usar o primeiro modelo LightGBM para feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': lgb_models[0].feature_importance(importance_type='gain')\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "print(\"\\nTop 30 Features:\")\n",
    "print(feature_importance.head(30))\n",
    "\n",
    "# Visualizar top 30 features\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(range(30), feature_importance['importance'].head(30))\n",
    "plt.yticks(range(30), feature_importance['feature'].head(30))\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.title('Top 30 Feature Importance - LightGBM')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Salvar feature importance\n",
    "feature_importance.to_csv('../data/processed/feature_importance_gpu.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c5daa",
   "metadata": {},
   "source": [
    "## Comparação dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARACAO DE MODELOS BASICOS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "results_comparison = pd.DataFrame([\n",
    "    {'Model': 'Logistic Regression', 'Mean AUC': lr_mean_auc, 'Std AUC': lr_std_auc},\n",
    "    {'Model': 'LightGBM (GPU)', 'Mean AUC': lgb_mean_auc, 'Std AUC': lgb_std_auc},\n",
    "    {'Model': 'XGBoost (GPU)', 'Mean AUC': xgb_mean_auc, 'Std AUC': xgb_std_auc},\n",
    "    {'Model': 'CatBoost (GPU)', 'Mean AUC': cat_mean_auc, 'Std AUC': cat_std_auc}\n",
    "])\n",
    "\n",
    "results_comparison = results_comparison.sort_values('Mean AUC', ascending=False)\n",
    "print(results_comparison)\n",
    "\n",
    "# Visualizar comparacao\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(results_comparison['Model'], results_comparison['Mean AUC'])\n",
    "plt.xlabel('Mean AUC')\n",
    "plt.title('Comparacao de Modelos - Cross Validation')\n",
    "plt.xlim(0.9, 1.0)\n",
    "for i, (auc, std) in enumerate(zip(results_comparison['Mean AUC'], results_comparison['Std AUC'])):\n",
    "    plt.text(auc, i, f' {auc:.5f} (+/- {std:.5f})', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7352b422",
   "metadata": {},
   "source": [
    "# Avaliando hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4962ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING COM OPTUNA\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6690a91",
   "metadata": {},
   "source": [
    "## Tuning LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b579961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - LIGHTGBM (GPU)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    \"\"\"Funcao objetivo para Optuna - LightGBM\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        \n",
    "        # Hiperparametros para otimizar\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        \n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "        val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[lgb.early_stopping(30, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "        auc = roc_auc_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Rodar otimizacao\n",
    "print(\"Iniciando Hyperparameter Tuning LightGBM...\")\n",
    "print(\"Isso pode levar alguns minutos...\\n\")\n",
    "\n",
    "study_lgb = optuna.create_study(direction='maximize', study_name='lgb_gpu_optimization')\n",
    "study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MELHORES HIPERPARAMETROS - LIGHTGBM\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Melhor AUC: {study_lgb.best_value:.5f}\")\n",
    "print(f\"Melhoria sobre baseline: {study_lgb.best_value - lgb_mean_auc:.5f}\")\n",
    "print(\"\\nMelhores parametros:\")\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d762e93",
   "metadata": {},
   "source": [
    "## Tuning XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - XGBOOST (GPU)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"Funcao objetivo para Optuna - XGBoost\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'gpu_id': 0,\n",
    "        \n",
    "        # Hiperparametros para otimizar\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0),\n",
    "        \n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'early_stopping_rounds': 30,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Rodar otimizacao\n",
    "print(\"Iniciando Hyperparameter Tuning XGBoost...\")\n",
    "print(\"Isso pode levar alguns minutos...\\n\")\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name='xgb_gpu_optimization')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MELHORES HIPERPARAMETROS - XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Melhor AUC: {study_xgb.best_value:.5f}\")\n",
    "print(f\"Melhoria sobre baseline: {study_xgb.best_value - xgb_mean_auc:.5f}\")\n",
    "print(\"\\nMelhores parametros:\")\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8bac8",
   "metadata": {},
   "source": [
    "## Tuning Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085938e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - CATBOOST (GPU)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def objective_cat(trial):\n",
    "    \"\"\"Funcao objetivo para Optuna - CatBoost\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'Logloss',\n",
    "        'task_type': 'GPU',\n",
    "        'devices': '0',\n",
    "        \n",
    "        # Hiperparametros para otimizar\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "        \n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_seed': RANDOM_STATE,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_val_fold, y_val_fold),\n",
    "            early_stopping_rounds=30,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Rodar otimizacao\n",
    "print(\"Iniciando Hyperparameter Tuning CatBoost...\")\n",
    "print(\"Isso pode levar alguns minutos...\\n\")\n",
    "\n",
    "study_cat = optuna.create_study(direction='maximize', study_name='cat_gpu_optimization')\n",
    "study_cat.optimize(objective_cat, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MELHORES HIPERPARAMETROS - CATBOOST\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Melhor AUC: {study_cat.best_value:.5f}\")\n",
    "print(f\"Melhoria sobre baseline: {study_cat.best_value - cat_mean_auc:.5f}\")\n",
    "print(\"\\nMelhores parametros:\")\n",
    "for key, value in study_cat.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece273a7",
   "metadata": {},
   "source": [
    "# Rodada final com hiperparâmetros otimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RODADAS FINAIS COM HIPERPARAMETROS OTIMIZADOS\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c23661",
   "metadata": {},
   "source": [
    "## Treino LightGBM com hiperparametros otimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TREINANDO LIGHTGBM OTIMIZADO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "lgb_best_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': -1,\n",
    "    **study_lgb.best_params\n",
    "}\n",
    "\n",
    "lgb_tuned_scores = []\n",
    "lgb_tuned_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Training Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    X_val_fold = X.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_best_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    \n",
    "    lgb_tuned_scores.append(auc)\n",
    "    lgb_tuned_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}, Best iteration: {model.best_iteration}\")\n",
    "\n",
    "lgb_tuned_mean_auc = np.mean(lgb_tuned_scores)\n",
    "lgb_tuned_std_auc = np.std(lgb_tuned_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"LightGBM Tuned Mean AUC: {lgb_tuned_mean_auc:.5f} (+/- {lgb_tuned_std_auc:.5f})\")\n",
    "print(f\"Improvement: {lgb_tuned_mean_auc - lgb_mean_auc:.5f}\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7dd61e",
   "metadata": {},
   "source": [
    "## Treino XGBoost com hiperparametros otimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caab5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TREINANDO XGBOOST OTIMIZADO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "xgb_best_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'n_jobs': -1,\n",
    "    **study_xgb.best_params\n",
    "}\n",
    "\n",
    "xgb_tuned_scores = []\n",
    "xgb_tuned_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Training Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    X_val_fold = X.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgb_best_params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_val_fold, y_val_fold)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    \n",
    "    xgb_tuned_scores.append(auc)\n",
    "    xgb_tuned_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}, Best iteration: {model.best_iteration}\")\n",
    "\n",
    "xgb_tuned_mean_auc = np.mean(xgb_tuned_scores)\n",
    "xgb_tuned_std_auc = np.std(xgb_tuned_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"XGBoost Tuned Mean AUC: {xgb_tuned_mean_auc:.5f} (+/- {xgb_tuned_std_auc:.5f})\")\n",
    "print(f\"Improvement: {xgb_tuned_mean_auc - xgb_mean_auc:.5f}\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d6e985",
   "metadata": {},
   "source": [
    "## Treino Catboost com hiperparametros otimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TREINANDO CATBOOST OTIMIZADO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "cat_best_params = {\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'Logloss',\n",
    "    'task_type': 'GPU',\n",
    "    'devices': '0',\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'verbose': False,\n",
    "    **study_cat.best_params\n",
    "}\n",
    "\n",
    "cat_tuned_scores = []\n",
    "cat_tuned_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Training Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    X_val_fold = X.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(**cat_best_params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_val_fold, y_val_fold),\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    \n",
    "    cat_tuned_scores.append(auc)\n",
    "    cat_tuned_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}, Best iteration: {model.best_iteration_}\")\n",
    "\n",
    "cat_tuned_mean_auc = np.mean(cat_tuned_scores)\n",
    "cat_tuned_std_auc = np.std(cat_tuned_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CatBoost Tuned Mean AUC: {cat_tuned_mean_auc:.5f} (+/- {cat_tuned_std_auc:.5f})\")\n",
    "print(f\"Improvement: {cat_tuned_mean_auc - cat_mean_auc:.5f}\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8de65c",
   "metadata": {},
   "source": [
    "# Análise com Top 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ff231",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ANALISE: TODAS FEATURES VS TOP 30 FEATURES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Carregar feature importance\n",
    "top_n = 30\n",
    "top_features = feature_importance.head(top_n)['feature'].tolist()\n",
    "\n",
    "print(f\"Top {top_n} features selecionadas:\")\n",
    "print(top_features)\n",
    "print()\n",
    "\n",
    "# Datasets com features selecionadas\n",
    "X_selected = X[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "\n",
    "# ===== LIGHTGBM: Todas vs Top 30 =====\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"LIGHTGBM: Comparacao Todas Features vs Top 30\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "lgb_top_scores = []\n",
    "lgb_top_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, y), 1):\n",
    "    X_train_fold = X_selected.iloc[train_idx]\n",
    "    X_val_fold = X_selected.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_best_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    lgb_top_scores.append(auc)\n",
    "    lgb_top_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC (top {top_n}): {auc:.5f}\")\n",
    "\n",
    "lgb_top_mean = np.mean(lgb_top_scores)\n",
    "lgb_top_std = np.std(lgb_top_scores)\n",
    "\n",
    "print(f\"\\nLightGBM - Todas features:  {lgb_tuned_mean_auc:.5f} (+/- {lgb_tuned_std_auc:.5f})\")\n",
    "print(f\"LightGBM - Top {top_n} features: {lgb_top_mean:.5f} (+/- {lgb_top_std:.5f})\")\n",
    "print(f\"Diferenca: {lgb_top_mean - lgb_tuned_mean_auc:.5f}\")\n",
    "\n",
    "\n",
    "# ===== XGBOOST: Todas vs Top 30 =====\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"XGBOOST: Comparacao Todas Features vs Top 30\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "xgb_top_scores = []\n",
    "xgb_top_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, y), 1):\n",
    "    X_train_fold = X_selected.iloc[train_idx]\n",
    "    X_val_fold = X_selected.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgb_best_params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_val_fold, y_val_fold)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    xgb_top_scores.append(auc)\n",
    "    xgb_top_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC (top {top_n}): {auc:.5f}\")\n",
    "\n",
    "xgb_top_mean = np.mean(xgb_top_scores)\n",
    "xgb_top_std = np.std(xgb_top_scores)\n",
    "\n",
    "print(f\"\\nXGBoost - Todas features:  {xgb_tuned_mean_auc:.5f} (+/- {xgb_tuned_std_auc:.5f})\")\n",
    "print(f\"XGBoost - Top {top_n} features: {xgb_top_mean:.5f} (+/- {xgb_top_std:.5f})\")\n",
    "print(f\"Diferenca: {xgb_top_mean - xgb_tuned_mean_auc:.5f}\")\n",
    "\n",
    "\n",
    "# ===== CATBOOST: Todas vs Top 30 =====\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CATBOOST: Comparacao Todas Features vs Top 30\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "cat_top_scores = []\n",
    "cat_top_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, y), 1):\n",
    "    X_train_fold = X_selected.iloc[train_idx]\n",
    "    X_val_fold = X_selected.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(**cat_best_params)\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_val_fold, y_val_fold),\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    cat_top_scores.append(auc)\n",
    "    cat_top_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC (top {top_n}): {auc:.5f}\")\n",
    "\n",
    "cat_top_mean = np.mean(cat_top_scores)\n",
    "cat_top_std = np.std(cat_top_scores)\n",
    "\n",
    "print(f\"\\nCatBoost - Todas features:  {cat_tuned_mean_auc:.5f} (+/- {cat_tuned_std_auc:.5f})\")\n",
    "print(f\"CatBoost - Top {top_n} features: {cat_top_mean:.5f} (+/- {cat_top_std:.5f})\")\n",
    "print(f\"Diferenca: {cat_top_mean - cat_tuned_mean_auc:.5f}\")\n",
    "\n",
    "\n",
    "# ===== RESUMO DA COMPARACAO =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMO: TODAS FEATURES VS TOP 30\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "comparison_features = pd.DataFrame([\n",
    "    {'Model': 'LightGBM', \n",
    "     'Todas Features': f\"{lgb_tuned_mean_auc:.5f}\", \n",
    "     f'Top {top_n} Features': f\"{lgb_top_mean:.5f}\",\n",
    "     'Diferenca': f\"{lgb_top_mean - lgb_tuned_mean_auc:.5f}\"},\n",
    "    {'Model': 'XGBoost', \n",
    "     'Todas Features': f\"{xgb_tuned_mean_auc:.5f}\", \n",
    "     f'Top {top_n} Features': f\"{xgb_top_mean:.5f}\",\n",
    "     'Diferenca': f\"{xgb_top_mean - xgb_tuned_mean_auc:.5f}\"},\n",
    "    {'Model': 'CatBoost', \n",
    "     'Todas Features': f\"{cat_tuned_mean_auc:.5f}\", \n",
    "     f'Top {top_n} Features': f\"{cat_top_mean:.5f}\",\n",
    "     'Diferenca': f\"{cat_top_mean - cat_tuned_mean_auc:.5f}\"}\n",
    "])\n",
    "\n",
    "print(comparison_features.to_string(index=False))\n",
    "\n",
    "# Visualizar comparacao\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(['LightGBM', 'XGBoost', 'CatBoost']))\n",
    "width = 0.35\n",
    "\n",
    "todas_features = [lgb_tuned_mean_auc, xgb_tuned_mean_auc, cat_tuned_mean_auc]\n",
    "top_features_scores = [lgb_top_mean, xgb_top_mean, cat_top_mean]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, todas_features, width, label='Todas Features', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, top_features_scores, width, label=f'Top {top_n} Features', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Mean AUC')\n",
    "ax.set_title('Comparacao: Todas Features vs Top 30 Features')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['LightGBM', 'XGBoost', 'CatBoost'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0.96, 0.98)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.5f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Decisao: usar todas features ou top 30?\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DECISAO: Qual conjunto de features usar no ensemble?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calcular media dos 3 modelos\n",
    "mean_all_features = np.mean([lgb_tuned_mean_auc, xgb_tuned_mean_auc, cat_tuned_mean_auc])\n",
    "mean_top_features = np.mean([lgb_top_mean, xgb_top_mean, cat_top_mean])\n",
    "\n",
    "print(f\"\\nMedia AUC (Todas features): {mean_all_features:.5f}\")\n",
    "print(f\"Media AUC (Top {top_n} features): {mean_top_features:.5f}\")\n",
    "print(f\"Diferenca: {mean_top_features - mean_all_features:.5f}\")\n",
    "\n",
    "if mean_top_features > mean_all_features:\n",
    "    print(f\"\\nCONCLUSAO: Top {top_n} features apresentam melhor performance!\")\n",
    "    print(f\"Usaremos top {top_n} features no ensemble final.\")\n",
    "    use_top_features = True\n",
    "else:\n",
    "    print(f\"\\nCONCLUSAO: Todas as features apresentam melhor performance!\")\n",
    "    print(f\"Usaremos todas as features no ensemble final.\")\n",
    "    use_top_features = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643067d",
   "metadata": {},
   "source": [
    "## Ensemble com top 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENSEMBLE COM TOP 30 FEATURES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Treinar todos os modelos otimizados com top 30 features\n",
    "ensemble_lgb_models = []\n",
    "ensemble_xgb_models = []\n",
    "ensemble_cat_models = []\n",
    "ensemble_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, y), 1):\n",
    "    print(f\"\\nTraining Ensemble Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold = X_selected.iloc[train_idx]\n",
    "    X_val_fold = X_selected.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    # LightGBM\n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    lgb_model = lgb.train(\n",
    "        lgb_best_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(**xgb_best_params)\n",
    "    xgb_model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_val_fold, y_val_fold)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # CatBoost\n",
    "    cat_model = CatBoostClassifier(**cat_best_params)\n",
    "    cat_model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_val_fold, y_val_fold),\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predicoes\n",
    "    lgb_pred = lgb_model.predict(X_val_fold, num_iteration=lgb_model.best_iteration)\n",
    "    xgb_pred = xgb_model.predict_proba(X_val_fold)[:, 1]\n",
    "    cat_pred = cat_model.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Ensemble: media simples\n",
    "    ensemble_pred = (lgb_pred + xgb_pred + cat_pred) / 3\n",
    "    \n",
    "    auc = roc_auc_score(y_val_fold, ensemble_pred)\n",
    "    ensemble_scores.append(auc)\n",
    "    \n",
    "    ensemble_lgb_models.append(lgb_model)\n",
    "    ensemble_xgb_models.append(xgb_model)\n",
    "    ensemble_cat_models.append(cat_model)\n",
    "    \n",
    "    print(f\"Fold {fold} Ensemble AUC: {auc:.5f}\")\n",
    "\n",
    "ensemble_mean_auc = np.mean(ensemble_scores)\n",
    "ensemble_std_auc = np.std(ensemble_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Ensemble Mean AUC: {ensemble_mean_auc:.5f} (+/- {ensemble_std_auc:.5f})\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a5054",
   "metadata": {},
   "source": [
    "## Comparação Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eaa69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARACAO FINAL DE TODOS OS MODELOS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "final_comparison = pd.DataFrame([\n",
    "    {'Model': 'Logistic Regression', 'Mean AUC': lr_mean_auc, 'Std AUC': lr_std_auc},\n",
    "    {'Model': 'LightGBM Baseline', 'Mean AUC': lgb_mean_auc, 'Std AUC': lgb_std_auc},\n",
    "    {'Model': 'XGBoost Baseline', 'Mean AUC': xgb_mean_auc, 'Std AUC': xgb_std_auc},\n",
    "    {'Model': 'CatBoost Baseline', 'Mean AUC': cat_mean_auc, 'Std AUC': cat_std_auc},\n",
    "    {'Model': 'LightGBM Tuned', 'Mean AUC': lgb_tuned_mean_auc, 'Std AUC': lgb_tuned_std_auc},\n",
    "    {'Model': 'XGBoost Tuned', 'Mean AUC': xgb_tuned_mean_auc, 'Std AUC': xgb_tuned_std_auc},\n",
    "    {'Model': 'CatBoost Tuned', 'Mean AUC': cat_tuned_mean_auc, 'Std AUC': cat_tuned_std_auc},\n",
    "    {'Model': 'Ensemble (Top 30)', 'Mean AUC': ensemble_mean_auc, 'Std AUC': ensemble_std_auc}\n",
    "])\n",
    "\n",
    "final_comparison = final_comparison.sort_values('Mean AUC', ascending=False)\n",
    "print(final_comparison.to_string(index=False))\n",
    "\n",
    "# Visualizar comparacao final\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red' if 'Baseline' in m or 'Logistic' in m else 'green' if 'Ensemble' in m else 'blue' \n",
    "          for m in final_comparison['Model']]\n",
    "plt.barh(final_comparison['Model'], final_comparison['Mean AUC'], color=colors, alpha=0.7)\n",
    "plt.xlabel('Mean AUC')\n",
    "plt.title('Comparacao Final de Todos os Modelos')\n",
    "plt.xlim(0.93, 0.98)\n",
    "for i, (auc, std) in enumerate(zip(final_comparison['Mean AUC'], final_comparison['Std AUC'])):\n",
    "    plt.text(auc, i, f' {auc:.5f} (+/- {std:.5f})', va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d4081",
   "metadata": {},
   "source": [
    "# Salvandos modelos e resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869be174",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SALVANDO MODELOS E RESULTADOS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Salvar modelos do ensemble\n",
    "joblib.dump(ensemble_lgb_models, '../models/ensemble_lgb_models_gpu.pkl')\n",
    "joblib.dump(ensemble_xgb_models, '../models/ensemble_xgb_models_gpu.pkl')\n",
    "joblib.dump(ensemble_cat_models, '../models/ensemble_cat_models_gpu.pkl')\n",
    "\n",
    "# Salvar top features\n",
    "with open('../data/processed/top_30_features.txt', 'w') as f:\n",
    "    for feature in top_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "# Salvar comparacao final\n",
    "final_comparison.to_csv('../data/processed/final_model_comparison_gpu.csv', index=False)\n",
    "\n",
    "print(\"Modelos e resultados salvos com sucesso!\")\n",
    "print(\"\\nArquivos salvos:\")\n",
    "print(\"  - ../models/ensemble_lgb_models_gpu.pkl\")\n",
    "print(\"  - ../models/ensemble_xgb_models_gpu.pkl\")\n",
    "print(\"  - ../models/ensemble_cat_models_gpu.pkl\")\n",
    "print(\"  - ../data/processed/top_30_features.txt\")\n",
    "print(\"  - ../data/processed/final_model_comparison_gpu.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELAGEM COMPLETA!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
