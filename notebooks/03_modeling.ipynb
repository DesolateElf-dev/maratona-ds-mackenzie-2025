{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663e0635",
   "metadata": {},
   "source": [
    "# Setup e Carregamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b62c4238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (750000, 69)\n",
      "Test shape: (250000, 68)\n",
      "\n",
      "X shape: (750000, 67)\n",
      "y shape: (750000,)\n",
      "X_test shape: (250000, 67)\n",
      "\n",
      "Distribuição do target:\n",
      "y\n",
      "0    659512\n",
      "1     90488\n",
      "Name: count, dtype: int64\n",
      "y\n",
      "0    0.879349\n",
      "1    0.120651\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Scale pos weight: 7.29\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Carregar dados processados\n",
    "train = pd.read_csv('../data/processed/train_processed.csv')\n",
    "test = pd.read_csv('../data/processed/test_processed.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Separar features e target\n",
    "y = train['y']\n",
    "X = train.drop(['y', 'id'], axis=1, errors='ignore')\n",
    "X_test = test.drop(['id'], axis=1, errors='ignore')\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Verificar desbalanceamento\n",
    "print(f\"\\nDistribuição do target:\")\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# Calcular scale_pos_weight para modelos tree-based\n",
    "scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"\\nScale pos weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfef6aa",
   "metadata": {},
   "source": [
    "# Analises de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b91b4",
   "metadata": {},
   "source": [
    "## Configuração da Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3979822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validação cruzada configurada\n"
     ]
    }
   ],
   "source": [
    "# Stratified K-Fold para manter proporção das classes\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "def evaluate_model_cv(model, X, y, cv=skf):\n",
    "    \"\"\"\n",
    "    Avalia modelo usando cross-validation\n",
    "    Retorna: média e desvio padrão do AUC-ROC\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Treinar\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Prever probabilidades\n",
    "        y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        # Calcular AUC\n",
    "        auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "        scores.append(auc)\n",
    "        \n",
    "        print(f\"Fold {fold}: AUC = {auc:.5f}\")\n",
    "    \n",
    "    mean_auc = np.mean(scores)\n",
    "    std_auc = np.std(scores)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Mean AUC: {mean_auc:.5f} (+/- {std_auc:.5f})\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return mean_auc, std_auc, scores\n",
    "\n",
    "print(\"✅ Validação cruzada configurada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e31252",
   "metadata": {},
   "source": [
    "## Baseline - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22eff86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODELO BASELINE: LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "\n",
      "Fold 1: AUC = 0.93547\n",
      "Fold 2: AUC = 0.93557\n",
      "Fold 3: AUC = 0.93407\n",
      "Fold 4: AUC = 0.93658\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m lr_model = LogisticRegression(\n\u001b[32m      7\u001b[39m     random_state=RANDOM_STATE,\n\u001b[32m      8\u001b[39m     max_iter=\u001b[32m1000\u001b[39m,\n\u001b[32m      9\u001b[39m     class_weight=\u001b[33m'\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     solver=\u001b[33m'\u001b[39m\u001b[33mlbfgs\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Avaliar com CV\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m lr_mean_auc, lr_std_auc, lr_scores = \u001b[43mevaluate_model_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Salvar resultado\u001b[39;00m\n\u001b[32m     17\u001b[39m baseline_results = {\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mLogistic Regression\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmean_auc\u001b[39m\u001b[33m'\u001b[39m: lr_mean_auc,\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mstd_auc\u001b[39m\u001b[33m'\u001b[39m: lr_std_auc,\n\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m'\u001b[39m: lr_scores\n\u001b[32m     22\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mevaluate_model_cv\u001b[39m\u001b[34m(model, X, y, cv)\u001b[39m\n\u001b[32m     13\u001b[39m y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Treinar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Prever probabilidades\u001b[39;00m\n\u001b[32m     19\u001b[39m y_pred_proba = model.predict_proba(X_val_fold)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1384\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1382\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1409\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1410\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:459\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    455\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / (C * sw_sum)\n\u001b[32m    456\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    457\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    458\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mftol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_get_additional_lbfgs_options_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    474\u001b[39m     solver,\n\u001b[32m    475\u001b[39m     opt_res,\n\u001b[32m    476\u001b[39m     max_iter,\n\u001b[32m    477\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    478\u001b[39m )\n\u001b[32m    479\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:784\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    781\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    782\u001b[39m                              **options)\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    787\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    788\u001b[39m                         **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:469\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[39m\n\u001b[32m    461\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    462\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    472\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:403\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:353\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28mself\u001b[39m._nfev += \u001b[32m1\u001b[39m\n\u001b[32m    355\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\scipy\\_lib\\_util.py:590\u001b[39m, in \u001b[36m_ScalarFunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    589\u001b[39m     \u001b[38;5;66;03m# The user of this class might want `x` to remain unchanged.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m     fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    593\u001b[39m     \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:80\u001b[39m, in \u001b[36mMemoizeJac.__call__\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, *args):\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:74\u001b[39m, in \u001b[36mMemoizeJac._compute_if_needed\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(x == \u001b[38;5;28mself\u001b[39m.x) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.x = np.asarray(x).copy()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     fg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.jac = fg[\u001b[32m1\u001b[39m]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._value = fg[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:332\u001b[39m, in \u001b[36mLinearModelLoss.loss_gradient\u001b[39m\u001b[34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[39m\n\u001b[32m    330\u001b[39m     grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit_intercept:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m         grad[-\u001b[32m1\u001b[39m] = grad_pointwise.sum()\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    334\u001b[39m     grad = np.empty((n_classes, n_dof), dtype=weights.dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:51\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     50\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO BASELINE: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Modelo com class_weight para lidar com desbalanceamento\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "# Avaliar com CV\n",
    "lr_mean_auc, lr_std_auc, lr_scores = evaluate_model_cv(lr_model, X, y)\n",
    "\n",
    "# Salvar resultado\n",
    "baseline_results = {\n",
    "    'model': 'Logistic Regression',\n",
    "    'mean_auc': lr_mean_auc,\n",
    "    'std_auc': lr_std_auc,\n",
    "    'scores': lr_scores\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cbfb5",
   "metadata": {},
   "source": [
    "## LightGBM - Modelo Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04226098",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO PRINCIPAL: LIGHTGBM\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Parâmetros iniciais (baseline)\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 20,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Treinar com CV manual para ter controle\n",
    "lgb_scores = []\n",
    "lgb_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Training Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Criar datasets LightGBM\n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    # Treinar\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Prever\n",
    "    y_pred_proba = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "    \n",
    "    lgb_scores.append(auc)\n",
    "    lgb_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}\\n\")\n",
    "\n",
    "lgb_mean_auc = np.mean(lgb_scores)\n",
    "lgb_std_auc = np.std(lgb_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"LightGBM Mean AUC: {lgb_mean_auc:.5f} (+/- {lgb_std_auc:.5f})\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# Salvar resultado\n",
    "lgb_results = {\n",
    "    'model': 'LightGBM',\n",
    "    'mean_auc': lgb_mean_auc,\n",
    "    'std_auc': lgb_std_auc,\n",
    "    'scores': lgb_scores,\n",
    "    'models': lgb_models\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d53c2",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70777ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular importância média across folds\n",
    "feature_importance = pd.DataFrame()\n",
    "feature_importance['feature'] = X.columns\n",
    "\n",
    "importances = []\n",
    "for model in lgb_models:\n",
    "    importances.append(model.feature_importance(importance_type='gain'))\n",
    "\n",
    "feature_importance['importance'] = np.mean(importances, axis=0)\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualizar top 20\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'].head(20)[::-1], \n",
    "         feature_importance['importance'].head(20)[::-1])\n",
    "plt.xlabel('Importance (Gain)', fontweight='bold')\n",
    "plt.title('Top 20 Features - LightGBM', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 20 Features Mais Importantes:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Salvar importâncias\n",
    "feature_importance.to_csv('../data/processed/feature_importance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f07b8",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2d225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODELO: XGBOOST\n",
      "======================================================================\n",
      "\n",
      "Training Fold 1/5\n",
      "[0]\ttrain-auc:0.94272\tvalid-auc:0.94247\n",
      "[100]\ttrain-auc:0.96068\tvalid-auc:0.96023\n",
      "[200]\ttrain-auc:0.96557\tvalid-auc:0.96421\n",
      "[300]\ttrain-auc:0.96798\tvalid-auc:0.96572\n",
      "[400]\ttrain-auc:0.96956\tvalid-auc:0.96645\n",
      "[500]\ttrain-auc:0.97075\tvalid-auc:0.96687\n",
      "[600]\ttrain-auc:0.97183\tvalid-auc:0.96723\n",
      "[700]\ttrain-auc:0.97274\tvalid-auc:0.96745\n",
      "[800]\ttrain-auc:0.97361\tvalid-auc:0.96763\n",
      "[900]\ttrain-auc:0.97445\tvalid-auc:0.96781\n",
      "[999]\ttrain-auc:0.97518\tvalid-auc:0.96792\n",
      "Fold 1 AUC: 0.96792\n",
      "\n",
      "Training Fold 2/5\n",
      "[0]\ttrain-auc:0.94194\tvalid-auc:0.94122\n",
      "[100]\ttrain-auc:0.96087\tvalid-auc:0.95890\n",
      "[200]\ttrain-auc:0.96579\tvalid-auc:0.96297\n",
      "[300]\ttrain-auc:0.96825\tvalid-auc:0.96457\n",
      "[400]\ttrain-auc:0.96980\tvalid-auc:0.96532\n",
      "[500]\ttrain-auc:0.97108\tvalid-auc:0.96582\n",
      "[600]\ttrain-auc:0.97210\tvalid-auc:0.96610\n",
      "[700]\ttrain-auc:0.97304\tvalid-auc:0.96634\n",
      "[800]\ttrain-auc:0.97392\tvalid-auc:0.96652\n",
      "[900]\ttrain-auc:0.97470\tvalid-auc:0.96663\n",
      "[999]\ttrain-auc:0.97543\tvalid-auc:0.96672\n",
      "Fold 2 AUC: 0.96672\n",
      "\n",
      "Training Fold 3/5\n",
      "[0]\ttrain-auc:0.94247\tvalid-auc:0.94139\n",
      "[100]\ttrain-auc:0.96092\tvalid-auc:0.95928\n",
      "[200]\ttrain-auc:0.96585\tvalid-auc:0.96334\n",
      "[300]\ttrain-auc:0.96827\tvalid-auc:0.96486\n",
      "[400]\ttrain-auc:0.96974\tvalid-auc:0.96549\n",
      "[500]\ttrain-auc:0.97104\tvalid-auc:0.96597\n",
      "[600]\ttrain-auc:0.97201\tvalid-auc:0.96620\n",
      "[700]\ttrain-auc:0.97300\tvalid-auc:0.96648\n",
      "[800]\ttrain-auc:0.97388\tvalid-auc:0.96667\n",
      "[900]\ttrain-auc:0.97466\tvalid-auc:0.96680\n",
      "[999]\ttrain-auc:0.97540\tvalid-auc:0.96689\n",
      "Fold 3 AUC: 0.96689\n",
      "\n",
      "Training Fold 4/5\n",
      "[0]\ttrain-auc:0.94143\tvalid-auc:0.94163\n",
      "[100]\ttrain-auc:0.96081\tvalid-auc:0.96040\n",
      "[200]\ttrain-auc:0.96568\tvalid-auc:0.96423\n",
      "[300]\ttrain-auc:0.96805\tvalid-auc:0.96568\n",
      "[400]\ttrain-auc:0.96965\tvalid-auc:0.96649\n",
      "[500]\ttrain-auc:0.97089\tvalid-auc:0.96694\n",
      "[600]\ttrain-auc:0.97194\tvalid-auc:0.96726\n",
      "[700]\ttrain-auc:0.97286\tvalid-auc:0.96749\n",
      "[800]\ttrain-auc:0.97370\tvalid-auc:0.96764\n",
      "[900]\ttrain-auc:0.97448\tvalid-auc:0.96777\n",
      "[999]\ttrain-auc:0.97519\tvalid-auc:0.96785\n",
      "Fold 4 AUC: 0.96785\n",
      "\n",
      "Training Fold 5/5\n",
      "[0]\ttrain-auc:0.94251\tvalid-auc:0.94191\n",
      "[100]\ttrain-auc:0.96067\tvalid-auc:0.95943\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m dval = xgb.DMatrix(X_val_fold, label=y_val_fold)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Treinar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m model = \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Prever\u001b[39;00m\n\u001b[32m     41\u001b[39m y_pred_proba = model.predict(dval, iteration_range=(\u001b[32m0\u001b[39m, model.best_iteration))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\xgboost\\training.py:200\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    199\u001b[39m     bst.update(dtrain, iteration=i, fobj=obj)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    203\u001b[39m bst = cb_container.after_training(bst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\xgboost\\callback.py:266\u001b[39m, in \u001b[36mCallbackContainer.after_iteration\u001b[39m\u001b[34m(self, model, epoch, dtrain, evals)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m name.find(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m) == -\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDataset name should not contain `-`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m score: \u001b[38;5;28mstr\u001b[39m = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_margin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m metric_score = _parse_eval_str(score)\n\u001b[32m    268\u001b[39m \u001b[38;5;28mself\u001b[39m._update_history(metric_score, epoch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emaci\\maratona-ds-mackenzie-2025\\.venv\\Lib\\site-packages\\xgboost\\core.py:2540\u001b[39m, in \u001b[36mBooster.eval_set\u001b[39m\u001b[34m(self, evals, iteration, feval, output_margin)\u001b[39m\n\u001b[32m   2537\u001b[39m evnames = c_array(ctypes.c_char_p, [c_str(d[\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals])\n\u001b[32m   2538\u001b[39m msg = ctypes.c_char_p()\n\u001b[32m   2539\u001b[39m _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2540\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterEvalOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc_bst_ulong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2548\u001b[39m )\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m msg.value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2550\u001b[39m res = msg.value.decode()  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO: XGBOOST\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "xgb_scores = []\n",
    "xgb_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Training Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Criar DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n",
    "    dval = xgb.DMatrix(X_val_fold, label=y_val_fold)\n",
    "    \n",
    "    # Treinar\n",
    "    model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'valid')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    # Prever\n",
    "    y_pred_proba = model.predict(dval, iteration_range=(0, model.best_iteration))\n",
    "    auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "    \n",
    "    xgb_scores.append(auc)\n",
    "    xgb_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}\\n\")\n",
    "\n",
    "xgb_mean_auc = np.mean(xgb_scores)\n",
    "xgb_std_auc = np.std(xgb_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"XGBoost Mean AUC: {xgb_mean_auc:.5f} (+/- {xgb_std_auc:.5f})\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "xgb_results = {\n",
    "    'model': 'XGBoost',\n",
    "    'mean_auc': xgb_mean_auc,\n",
    "    'std_auc': xgb_std_auc,\n",
    "    'scores': xgb_scores,\n",
    "    'models': xgb_models\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ff99d",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62205494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU disponível e será utilizada para treinamento\n",
      "\n",
      "======================================================================\n",
      "MODELO: CATBOOST (GPU)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Training Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC: 0.96596\n",
      "\n",
      "Training Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 AUC: 0.96486\n",
      "\n",
      "Training Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 AUC: 0.96489\n",
      "\n",
      "Training Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 AUC: 0.96574\n",
      "\n",
      "Training Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 AUC: 0.96525\n",
      "\n",
      "==================================================\n",
      "CatBoost (GPU) Mean AUC: 0.96534 (+/- 0.00045)\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "\n",
    "# Verificar se GPU está disponível\n",
    "try:\n",
    "    # Tenta criar um modelo simples com GPU\n",
    "    test_model = CatBoostClassifier(task_type='GPU', devices='0', iterations=1, verbose=False)\n",
    "    print(\"✓ GPU disponível e será utilizada para treinamento\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ GPU não disponível ou erro: {str(e)}\")\n",
    "    print(\"O treinamento continuará em CPU\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO: CATBOOST (GPU)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "cat_params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'AUC',\n",
    "    'task_type': 'GPU',              # Ativa GPU\n",
    "    'devices': '0',                  # ID da GPU (0 para primeira GPU)\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'verbose': 100,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "cat_scores = []\n",
    "cat_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\nTraining Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(**cat_params)\n",
    "    \n",
    "    # Treinar\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=(X_val_fold, y_val_fold),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Prever\n",
    "    y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "    auc = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "    \n",
    "    cat_scores.append(auc)\n",
    "    cat_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}\")\n",
    "\n",
    "cat_mean_auc = np.mean(cat_scores)\n",
    "cat_std_auc = np.std(cat_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CatBoost (GPU) Mean AUC: {cat_mean_auc:.5f} (+/- {cat_std_auc:.5f})\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "cat_results = {\n",
    "    'model': 'CatBoost (GPU)',\n",
    "    'mean_auc': cat_mean_auc,\n",
    "    'std_auc': cat_std_auc,\n",
    "    'scores': cat_scores,\n",
    "    'models': cat_models\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d432958",
   "metadata": {},
   "source": [
    "## Comparação de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_comparison = pd.DataFrame([\n",
    "    baseline_results,\n",
    "    lgb_results,\n",
    "    xgb_results,\n",
    "    cat_results\n",
    "])[['model', 'mean_auc', 'std_auc']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARAÇÃO DE MODELOS\")\n",
    "print(\"=\"*70)\n",
    "print(results_comparison.to_string(index=False))\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(results_comparison['model'], results_comparison['mean_auc'], \n",
    "         xerr=results_comparison['std_auc'], capsize=5, alpha=0.7)\n",
    "plt.xlabel('Mean AUC-ROC', fontweight='bold')\n",
    "plt.title('Comparação de Performance dos Modelos', fontsize=14, fontweight='bold')\n",
    "plt.xlim(0.85, results_comparison['mean_auc'].max() + 0.02)\n",
    "for idx, row in results_comparison.iterrows():\n",
    "    plt.text(row['mean_auc'], idx, f\" {row['mean_auc']:.5f}\", \n",
    "             va='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar melhor modelo\n",
    "best_model_name = results_comparison.loc[results_comparison['mean_auc'].idxmax(), 'model']\n",
    "print(f\"🏆 Melhor Modelo: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876052b",
   "metadata": {},
   "source": [
    "# Gerar Predições para Submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(models, X_test, model_name='ensemble'):\n",
    "    \"\"\"Gera predições usando média dos modelos (ensemble)\"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        if model_name == 'LightGBM':\n",
    "            pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        elif model_name == 'XGBoost':\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            pred = model.predict(dtest, iteration_range=(0, model.best_iteration))\n",
    "        else:  # CatBoost\n",
    "            pred = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Média das predições (ensemble dos folds)\n",
    "    final_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# Gerar predições para cada modelo\n",
    "print(\"\\nGerando predições...\")\n",
    "\n",
    "lgb_predictions = generate_predictions(lgb_models, X_test, 'LightGBM')\n",
    "xgb_predictions = generate_predictions(xgb_models, X_test, 'XGBoost')\n",
    "cat_predictions = generate_predictions(cat_models, X_test, 'CatBoost')\n",
    "\n",
    "# Ensemble final: média ponderada pelos scores de CV\n",
    "total_score = lgb_mean_auc + xgb_mean_auc + cat_mean_auc\n",
    "lgb_weight = lgb_mean_auc / total_score\n",
    "xgb_weight = xgb_mean_auc / total_score\n",
    "cat_weight = cat_mean_auc / total_score\n",
    "\n",
    "ensemble_predictions = (lgb_weight * lgb_predictions + \n",
    "                        xgb_weight * xgb_predictions + \n",
    "                        cat_weight * cat_predictions)\n",
    "\n",
    "print(f\"\\nPesos do ensemble:\")\n",
    "print(f\"  LightGBM: {lgb_weight:.3f}\")\n",
    "print(f\"  XGBoost: {xgb_weight:.3f}\")\n",
    "print(f\"  CatBoost: {cat_weight:.3f}\")\n",
    "\n",
    "# Criar submission files\n",
    "test_ids = test['id'].values if 'id' in test.columns else range(len(test))\n",
    "\n",
    "submissions = {\n",
    "    'lgb': lgb_predictions,\n",
    "    'xgb': xgb_predictions,\n",
    "    'cat': cat_predictions,\n",
    "    'ensemble': ensemble_predictions\n",
    "}\n",
    "\n",
    "for name, preds in submissions.items():\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'y': preds\n",
    "    })\n",
    "    submission.to_csv(f'../submissions/submission_{name}.csv', index=False)\n",
    "    print(f\"✅ Arquivo salvo: submission_{name}.csv\")\n",
    "\n",
    "print(\"\\n🎯 Submissões prontas! Recomendo testar o ensemble primeiro.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569e000",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f29bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar predições dos 3 modelos (primeiros 10 clientes)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'id': test_ids[:10],\n",
    "    'LightGBM': lgb_predictions[:10],\n",
    "    'XGBoost': xgb_predictions[:10],\n",
    "    'CatBoost': cat_predictions[:10],\n",
    "    'Ensemble': ensemble_predictions[:10]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARAÇÃO DE PREDIÇÕES - Primeiros 10 Clientes\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calcular diferenças\n",
    "comparison_df['diff_lgb_xgb'] = abs(comparison_df['LightGBM'] - comparison_df['XGBoost'])\n",
    "comparison_df['diff_lgb_cat'] = abs(comparison_df['LightGBM'] - comparison_df['CatBoost'])\n",
    "comparison_df['diff_xgb_cat'] = abs(comparison_df['XGBoost'] - comparison_df['CatBoost'])\n",
    "\n",
    "print(f\"\\nMédia das diferenças entre modelos:\")\n",
    "print(f\"  LightGBM vs XGBoost: {comparison_df['diff_lgb_xgb'].mean():.5f}\")\n",
    "print(f\"  LightGBM vs CatBoost: {comparison_df['diff_lgb_cat'].mean():.5f}\")\n",
    "print(f\"  XGBoost vs CatBoost: {comparison_df['diff_xgb_cat'].mean():.5f}\")\n",
    "\n",
    "# Visualizar distribuição das predições\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].hist(lgb_predictions, bins=50, alpha=0.7, label='LightGBM', color='blue')\n",
    "axes[0, 0].set_title('Distribuição - LightGBM', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Probabilidade Predita')\n",
    "axes[0, 0].axvline(lgb_predictions.mean(), color='red', linestyle='--', \n",
    "                    label=f'Média: {lgb_predictions.mean():.3f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].hist(xgb_predictions, bins=50, alpha=0.7, label='XGBoost', color='green')\n",
    "axes[0, 1].set_title('Distribuição - XGBoost', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Probabilidade Predita')\n",
    "axes[0, 1].axvline(xgb_predictions.mean(), color='red', linestyle='--',\n",
    "                    label=f'Média: {xgb_predictions.mean():.3f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "axes[1, 0].hist(cat_predictions, bins=50, alpha=0.7, label='CatBoost', color='orange')\n",
    "axes[1, 0].set_title('Distribuição - CatBoost', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Probabilidade Predita')\n",
    "axes[1, 0].axvline(cat_predictions.mean(), color='red', linestyle='--',\n",
    "                    label=f'Média: {cat_predictions.mean():.3f}')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].hist(ensemble_predictions, bins=50, alpha=0.7, label='Ensemble', color='purple')\n",
    "axes[1, 1].set_title('Distribuição - Ensemble (Final)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Probabilidade Predita')\n",
    "axes[1, 1].axvline(ensemble_predictions.mean(), color='red', linestyle='--',\n",
    "                    label=f'Média: {ensemble_predictions.mean():.3f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estatísticas das predições\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ESTATÍSTICAS DAS PREDIÇÕES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLightGBM:\")\n",
    "print(f\"  Média: {lgb_predictions.mean():.5f}\")\n",
    "print(f\"  Min: {lgb_predictions.min():.5f}\")\n",
    "print(f\"  Max: {lgb_predictions.max():.5f}\")\n",
    "print(f\"  Std: {lgb_predictions.std():.5f}\")\n",
    "\n",
    "print(f\"\\nXGBoost:\")\n",
    "print(f\"  Média: {xgb_predictions.mean():.5f}\")\n",
    "print(f\"  Min: {xgb_predictions.min():.5f}\")\n",
    "print(f\"  Max: {xgb_predictions.max():.5f}\")\n",
    "print(f\"  Std: {xgb_predictions.std():.5f}\")\n",
    "\n",
    "print(f\"\\nCatBoost:\")\n",
    "print(f\"  Média: {cat_predictions.mean():.5f}\")\n",
    "print(f\"  Min: {cat_predictions.min():.5f}\")\n",
    "print(f\"  Max: {cat_predictions.max():.5f}\")\n",
    "print(f\"  Std: {cat_predictions.std():.5f}\")\n",
    "\n",
    "print(f\"\\n🎯 ENSEMBLE (FINAL):\")\n",
    "print(f\"  Média: {ensemble_predictions.mean():.5f}\")\n",
    "print(f\"  Min: {ensemble_predictions.min():.5f}\")\n",
    "print(f\"  Max: {ensemble_predictions.max():.5f}\")\n",
    "print(f\"  Std: {ensemble_predictions.std():.5f}\")\n",
    "\n",
    "# Correlação entre modelos\n",
    "corr_matrix = pd.DataFrame({\n",
    "    'LightGBM': lgb_predictions,\n",
    "    'XGBoost': xgb_predictions,\n",
    "    'CatBoost': cat_predictions\n",
    "}).corr()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELAÇÃO ENTRE MODELOS\")\n",
    "print(\"=\"*70)\n",
    "print(corr_matrix)\n",
    "print(\"\\n💡 Correlação alta (>0.95) = modelos muito similares\")\n",
    "print(\"💡 Correlação moderada (0.85-0.95) = ideal para ensemble!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d2029",
   "metadata": {},
   "source": [
    "# Outros testes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d88de",
   "metadata": {},
   "source": [
    "## Teste sem duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ab74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTE: MODELOS SEM DURATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Remover duration\n",
    "X_no_duration = X.drop('duration', axis=1, errors='ignore')\n",
    "X_test_no_duration = X_test.drop('duration', axis=1, errors='ignore')\n",
    "\n",
    "print(f\"Features sem duration: {X_no_duration.shape[1]}\")\n",
    "\n",
    "# Treinar LightGBM sem duration\n",
    "lgb_scores_no_dur = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_no_duration, y), 1):\n",
    "    X_train_fold = X_no_duration.iloc[train_idx]\n",
    "    X_val_fold = X_no_duration.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    lgb_scores_no_dur.append(auc)\n",
    "    print(f\"Fold {fold} AUC (sem duration): {auc:.5f}\")\n",
    "\n",
    "print(f\"\\nMean AUC sem duration: {np.mean(lgb_scores_no_dur):.5f}\")\n",
    "print(f\"Comparação:\")\n",
    "print(f\"  COM duration: {lgb_mean_auc:.5f}\")\n",
    "print(f\"  SEM duration: {np.mean(lgb_scores_no_dur):.5f}\")\n",
    "print(f\"  Diferença: {lgb_mean_auc - np.mean(lgb_scores_no_dur):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2fe3b",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning com Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6072939",
   "metadata": {},
   "source": [
    "### LGB Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5eac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Função objetivo para Optuna\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "        val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[lgb.early_stopping(30, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "        auc = roc_auc_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Rodar otimização\n",
    "print(\"\\n🔧 Iniciando Hyperparameter Tuning...\")\n",
    "print(\"Isso pode levar alguns minutos...\\n\")\n",
    "\n",
    "study = optuna.create_study(direction='maximize', study_name='lgb_optimization')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MELHORES HIPERPARÂMETROS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMelhor AUC: {study.best_value:.5f}\")\n",
    "print(f\"Melhoria sobre baseline: +{study.best_value - lgb_mean_auc:.5f}\")\n",
    "print(\"\\nMelhores parâmetros:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847498b2",
   "metadata": {},
   "source": [
    "### XGB Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a72079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HYPERPARAMETER TUNING - XGBOOST (CORRIGIDO)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"Função objetivo para Optuna - XGBoost\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # Hiperparâmetros para otimizar\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0),\n",
    "        \n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'early_stopping_rounds': 30,  # MOVIDO PARA AQUI\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Criar study e otimizar\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name='xgb_optimization')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MELHORES HIPERPARÂMETROS - XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Melhor AUC: {study_xgb.best_value:.5f}\")\n",
    "print(f\"Melhoria sobre baseline: {study_xgb.best_value - xgb_mean_auc:.5f}\")\n",
    "print(\"\\nMelhores parâmetros:\")\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800a6f6",
   "metadata": {},
   "source": [
    "### Cat Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e49ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - CATBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def objective_cat(trial):\n",
    "    \"\"\"Função objetivo para Optuna - CatBoost\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'AUC',\n",
    "        'task_type': 'CPU',\n",
    "        \n",
    "        # Hiperparâmetros para otimizar\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "        \n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_val_fold, y_val_fold),\n",
    "            early_stopping_rounds=30,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Criar study e otimizar\n",
    "study_cat = optuna.create_study(direction='maximize', study_name='cat_optimization')\n",
    "study_cat.optimize(objective_cat, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MELHORES HIPERPARÂMETROS - CATBOOST\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Melhor AUC: {study_cat.best_value:.5f}\")\n",
    "print(f\"Melhoria sobre baseline: {study_cat.best_value - cat_mean_auc:.5f}\")\n",
    "print(\"\\nMelhores parâmetros:\")\n",
    "for key, value in study_cat.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7138ad",
   "metadata": {},
   "source": [
    "### Cat Tuning with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - CATBOOST (GPU)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def objective_cat(trial):\n",
    "    \"\"\"Função objetivo para Optuna - CatBoost com GPU\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'AUC',\n",
    "        'task_type': 'GPU',      # GPU ativada\n",
    "        'devices': '0',           # Primeira GPU\n",
    "        \n",
    "        # Hiperparâmetros para otimizar\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "        \n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_val_fold, y_val_fold),\n",
    "            early_stopping_rounds=30,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(auc)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Criar study e otimizar\n",
    "study_cat = optuna.create_study(direction='maximize', study_name='cat_gpu_optimization')\n",
    "study_cat.optimize(objective_cat, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MELHORES HIPERPARÂMETROS - CATBOOST (GPU)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Melhor AUC: {study_cat.best_value:.5f}\")\n",
    "print(f\"Melhoria sobre baseline: {study_cat.best_value - cat_mean_auc:.5f}\")\n",
    "print(\"\\nMelhores parâmetros:\")\n",
    "for key, value in study_cat.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e966aef0",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar feature importance\n",
    "feature_importance = pd.read_csv('../data/processed/feature_importance.csv')\n",
    "\n",
    "# Testar com top 30 features\n",
    "top_n = 30\n",
    "top_features = feature_importance.head(top_n)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nTestando com top {top_n} features:\")\n",
    "print(top_features)\n",
    "\n",
    "X_selected = X[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "# Treinar com features selecionadas\n",
    "lgb_scores_selected = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_selected, y), 1):\n",
    "    X_train_fold = X_selected.iloc[train_idx]\n",
    "    X_val_fold = X_selected.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    lgb_scores_selected.append(auc)\n",
    "    print(f\"Fold {fold} AUC (top {top_n}): {auc:.5f}\")\n",
    "\n",
    "print(f\"\\nMean AUC (top {top_n}): {np.mean(lgb_scores_selected):.5f}\")\n",
    "print(f\"Comparação com todas features: {lgb_mean_auc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99d56d",
   "metadata": {},
   "source": [
    "# Rodadas finais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657438d",
   "metadata": {},
   "source": [
    "## Hiperparâmetros otimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23188036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TREINANDO MODELO FINAL COM HIPERPARÂMETROS OTIMIZADOS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Melhores parâmetros do Optuna\n",
    "best_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 88,\n",
    "    'learning_rate': 0.0679235909913875,\n",
    "    'feature_fraction': 0.6008994544769531,\n",
    "    'bagging_fraction': 0.9617438469454634,\n",
    "    'bagging_freq': 1,\n",
    "    'min_child_samples': 48,\n",
    "    'max_depth': 12,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Treinar com CV\n",
    "tuned_models = []\n",
    "tuned_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Training Fold {fold}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    X_val_fold = X.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,  # Mais iterações\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "            lgb.log_evaluation(period=200)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    \n",
    "    tuned_scores.append(auc)\n",
    "    tuned_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}\")\n",
    "    print(f\"Best iteration: {model.best_iteration}\\n\")\n",
    "\n",
    "tuned_mean_auc = np.mean(tuned_scores)\n",
    "tuned_std_auc = np.std(tuned_scores)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TUNED MODEL - Mean AUC: {tuned_mean_auc:.5f} (+/- {tuned_std_auc:.5f})\")\n",
    "print(f\"Baseline Model - Mean AUC: {lgb_mean_auc:.5f}\")\n",
    "print(f\"Improvement: +{tuned_mean_auc - lgb_mean_auc:.5f}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Gerar predições\n",
    "tuned_predictions = []\n",
    "for model in tuned_models:\n",
    "    pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    tuned_predictions.append(pred)\n",
    "\n",
    "tuned_final_pred = np.mean(tuned_predictions, axis=0)\n",
    "\n",
    "# Salvar submissão\n",
    "submission_tuned = pd.DataFrame({\n",
    "    'id': test['id'].values if 'id' in test.columns else range(len(test)),\n",
    "    'y': tuned_final_pred\n",
    "})\n",
    "submission_tuned.to_csv('../submissions/submission_lgb_tuned.csv', index=False)\n",
    "print(\"✅ Submissão salva: submission_lgb_tuned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c252c",
   "metadata": {},
   "source": [
    "## Ensemble Otimizado (LightGBM Tuned + XGBoost + CatBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CRIANDO ENSEMBLE FINAL\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Pesos baseados em performance\n",
    "models_performance = {\n",
    "    'tuned_lgb': tuned_mean_auc,\n",
    "    'xgb': xgb_mean_auc,\n",
    "    'cat': cat_mean_auc\n",
    "}\n",
    "\n",
    "print(\"Performance dos modelos:\")\n",
    "for name, score in models_performance.items():\n",
    "    print(f\"  {name}: {score:.5f}\")\n",
    "\n",
    "# Calcular pesos\n",
    "total_score = sum(models_performance.values())\n",
    "weights = {k: v/total_score for k, v in models_performance.items()}\n",
    "\n",
    "print(\"\\nPesos do ensemble:\")\n",
    "for name, weight in weights.items():\n",
    "    print(f\"  {name}: {weight:.4f}\")\n",
    "\n",
    "# Ensemble ponderado\n",
    "ensemble_final = (\n",
    "    weights['tuned_lgb'] * tuned_final_pred + \n",
    "    weights['xgb'] * xgb_predictions + \n",
    "    weights['cat'] * cat_predictions\n",
    ")\n",
    "\n",
    "# Salvar\n",
    "submission_ensemble = pd.DataFrame({\n",
    "    'id': test['id'].values if 'id' in test.columns else range(len(test)),\n",
    "    'y': ensemble_final\n",
    "})\n",
    "submission_ensemble.to_csv('../submissions/submission_ensemble_optimized.csv', index=False)\n",
    "print(\"\\n✅ Ensemble otimizado salvo: submission_ensemble_optimized.csv\")\n",
    "\n",
    "# Comparação das distribuições\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARAÇÃO DAS PREDIÇÕES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLightGBM Tuned:\")\n",
    "print(f\"  Média: {tuned_final_pred.mean():.5f}\")\n",
    "print(f\"  Min: {tuned_final_pred.min():.5f}\")\n",
    "print(f\"  Max: {tuned_final_pred.max():.5f}\")\n",
    "\n",
    "print(f\"\\nEnsemble Otimizado:\")\n",
    "print(f\"  Média: {ensemble_final.mean():.5f}\")\n",
    "print(f\"  Min: {ensemble_final.min():.5f}\")\n",
    "print(f\"  Max: {ensemble_final.max():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f06e1c",
   "metadata": {},
   "source": [
    "## Top 30 Features com Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO SIMPLIFICADO: TOP 30 FEATURES + TUNING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Top features\n",
    "feature_importance = pd.read_csv('../data/processed/feature_importance.csv')\n",
    "top_30_features = feature_importance.head(30)['feature'].tolist()\n",
    "\n",
    "X_top30 = X[top_30_features]\n",
    "X_test_top30 = X_test[top_30_features]\n",
    "\n",
    "# Treinar com parâmetros otimizados\n",
    "top30_models = []\n",
    "top30_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_top30, y), 1):\n",
    "    X_train_fold = X_top30.iloc[train_idx]\n",
    "    X_val_fold = X_top30.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    \n",
    "    top30_scores.append(auc)\n",
    "    top30_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}\")\n",
    "\n",
    "top30_mean_auc = np.mean(top30_scores)\n",
    "print(f\"\\nTop 30 Features + Tuning: {top30_mean_auc:.5f}\")\n",
    "print(f\"All Features + Tuning: {tuned_mean_auc:.5f}\")\n",
    "print(f\"Diferença: {tuned_mean_auc - top30_mean_auc:.5f}\")\n",
    "\n",
    "# Se a diferença for pequena (<0.0005), vale a pena submeter a versão simplificada\n",
    "if abs(tuned_mean_auc - top30_mean_auc) < 0.0005:\n",
    "    print(\"\\n💡 Modelo simplificado tem performance similar! Gerando submissão...\")\n",
    "    \n",
    "    top30_predictions = []\n",
    "    for model in top30_models:\n",
    "        pred = model.predict(X_test_top30, num_iteration=model.best_iteration)\n",
    "        top30_predictions.append(pred)\n",
    "    \n",
    "    top30_final = np.mean(top30_predictions, axis=0)\n",
    "    \n",
    "    submission_top30 = pd.DataFrame({\n",
    "        'id': test['id'].values if 'id' in test.columns else range(len(test)),\n",
    "        'y': top30_final\n",
    "    })\n",
    "    submission_top30.to_csv('../submissions/submission_lgb_top30.csv', index=False)\n",
    "    print(\"✅ Submissão top30 salva!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c09890",
   "metadata": {},
   "source": [
    "# Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7fb18a",
   "metadata": {},
   "source": [
    "## Validações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDAÇÃO DOS ARQUIVOS DE SUBMISSÃO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Arquivos para validar\n",
    "submission_files = [\n",
    "    '../submissions/submission_lgb_top30.csv',\n",
    "    '../submissions/submission_lgb_tuned.csv',\n",
    "    '../submissions/submission_ensemble_optimized.csv'\n",
    "]\n",
    "\n",
    "# Carregar sample submission para referência\n",
    "sample_sub = pd.read_csv('../data/raw/analise-preditiva-de-comportamento-bancario/sample_submission.csv')\n",
    "\n",
    "for file_path in submission_files:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"\\n📄 Validando: {os.path.basename(file_path)}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Carregar submissão\n",
    "        sub = pd.read_csv(file_path)\n",
    "        \n",
    "        # Verificações\n",
    "        checks = {\n",
    "            'Tem coluna id': 'id' in sub.columns,\n",
    "            'Tem coluna y': 'y' in sub.columns,\n",
    "            'Número correto de linhas': len(sub) == len(sample_sub),\n",
    "            'Sem valores faltantes': sub.isnull().sum().sum() == 0,\n",
    "            'y entre 0 e 1': (sub['y'].min() >= 0) and (sub['y'].max() <= 1),\n",
    "            'IDs corretos': (sub['id'] == sample_sub['id']).all() if 'id' in sample_sub.columns else True\n",
    "        }\n",
    "        \n",
    "        # Exibir resultados\n",
    "        all_passed = True\n",
    "        for check, result in checks.items():\n",
    "            status = \"✅\" if result else \"❌\"\n",
    "            print(f\"{status} {check}: {result}\")\n",
    "            if not result:\n",
    "                all_passed = False\n",
    "        \n",
    "        # Estatísticas\n",
    "        print(f\"\\n📊 Estatísticas da coluna 'y':\")\n",
    "        print(f\"   Média: {sub['y'].mean():.5f}\")\n",
    "        print(f\"   Mediana: {sub['y'].median():.5f}\")\n",
    "        print(f\"   Min: {sub['y'].min():.5f}\")\n",
    "        print(f\"   Max: {sub['y'].max():.5f}\")\n",
    "        print(f\"   Desvio padrão: {sub['y'].std():.5f}\")\n",
    "        \n",
    "        if all_passed:\n",
    "            print(f\"\\n✅ {os.path.basename(file_path)} está PRONTO para submissão!\")\n",
    "        else:\n",
    "            print(f\"\\n❌ {os.path.basename(file_path)} tem problemas! Corrija antes de submeter.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Arquivo não encontrado: {file_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDAÇÃO CONCLUÍDA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Comparar distribuições entre submissões\n",
    "print(\"\\n📊 COMPARAÇÃO ENTRE SUBMISSÕES\\n\")\n",
    "\n",
    "sub1 = pd.read_csv('../submissions/submission_lgb_top30.csv')\n",
    "sub2 = pd.read_csv('../submissions/submission_lgb_tuned.csv')\n",
    "sub3 = pd.read_csv('../submissions/submission_ensemble_optimized.csv')\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Top30': [sub1['y'].mean(), sub1['y'].std(), sub1['y'].min(), sub1['y'].max()],\n",
    "    'Tuned': [sub2['y'].mean(), sub2['y'].std(), sub2['y'].min(), sub2['y'].max()],\n",
    "    'Ensemble': [sub3['y'].mean(), sub3['y'].std(), sub3['y'].min(), sub3['y'].max()]\n",
    "}, index=['Média', 'Desvio', 'Min', 'Max'])\n",
    "\n",
    "print(comparison.to_string())\n",
    "\n",
    "# Correlação entre predições\n",
    "print(\"\\n🔗 CORRELAÇÃO ENTRE PREDIÇÕES:\\n\")\n",
    "corr = pd.DataFrame({\n",
    "    'Top30': sub1['y'],\n",
    "    'Tuned': sub2['y'],\n",
    "    'Ensemble': sub3['y']\n",
    "}).corr()\n",
    "print(corr.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb57eb",
   "metadata": {},
   "source": [
    "## Conclusões e insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeeb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSÕES DO PROJETO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"📊 PERFORMANCE DOS MODELOS:\")\n",
    "print(f\"   • Logistic Regression (baseline): {lr_mean_auc:.5f}\")\n",
    "print(f\"   • LightGBM (baseline):           {lgb_mean_auc:.5f}\")\n",
    "print(f\"   • LightGBM (tuned):              {tuned_mean_auc:.5f}\")\n",
    "print(f\"   • XGBoost:                       {xgb_mean_auc:.5f}\")\n",
    "print(f\"   • CatBoost:                      {cat_mean_auc:.5f}\")\n",
    "\n",
    "print(\"\\n🎯 MELHOR MODELO: LightGBM com Hyperparameter Tuning\")\n",
    "print(f\"   • AUC-ROC: {tuned_mean_auc:.5f} (±{tuned_std_auc:.5f})\")\n",
    "print(f\"   • Melhoria sobre baseline: +{tuned_mean_auc - lgb_mean_auc:.5f}\")\n",
    "\n",
    "print(\"\\n💡 PRINCIPAIS INSIGHTS:\")\n",
    "print(\"   1. Duration é a feature mais importante (correlação 0.52 com target)\")\n",
    "print(\"   2. Histórico de campanha (poutcome='success') tem 76% de conversão\")\n",
    "print(\"   3. Meses mar, sep, oct, dec têm conversão >50% (alta sazonalidade)\")\n",
    "print(\"   4. Top 30 features capturam 99.99% da performance\")\n",
    "print(\"   5. Modelos tree-based superam modelos lineares significativamente\")\n",
    "\n",
    "print(\"\\n⚙️ TÉCNICAS APLICADAS:\")\n",
    "print(\"   • Feature Engineering: 51 novas features criadas\")\n",
    "print(\"   • Target Encoding para variáveis de alta cardinalidade\")\n",
    "print(\"   • Stratified K-Fold (5 folds) para validação\")\n",
    "print(\"   • Scale pos weight para lidar com desbalanceamento\")\n",
    "print(\"   • Hyperparameter Tuning com Optuna (50 trials)\")\n",
    "print(\"   • Ensemble de múltiplos modelos\")\n",
    "\n",
    "print(\"\\n📁 ARQUIVOS DE SUBMISSÃO GERADOS:\")\n",
    "print(\"   1. submission_lgb_top30.csv (recomendado - menos overfitting)\")\n",
    "print(\"   2. submission_lgb_tuned.csv (melhor CV score)\")\n",
    "print(\"   3. submission_ensemble_optimized.csv (mais robusto)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJETO CONCLUÍDO COM SUCESSO!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac82fb",
   "metadata": {},
   "source": [
    "## Última Submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GERANDO SUBMISSÕES CALIBRADAS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Carregar dados\n",
    "train = pd.read_csv('../data/processed/train_processed.csv')\n",
    "test = pd.read_csv('../data/processed/test_processed.csv')\n",
    "\n",
    "y = train['y']\n",
    "X = train.drop(['y', 'id'], axis=1, errors='ignore')\n",
    "X_test = test.drop(['id'], axis=1, errors='ignore')\n",
    "\n",
    "# Parâmetros otimizados\n",
    "best_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 88,\n",
    "    'learning_rate': 0.0679235909913875,\n",
    "    'feature_fraction': 0.6008994544769531,\n",
    "    'bagging_fraction': 0.9617438469454634,\n",
    "    'bagging_freq': 1,\n",
    "    'min_child_samples': 48,\n",
    "    'max_depth': 12,\n",
    "    'scale_pos_weight': (y == 0).sum() / (y == 1).sum(),\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Armazenar calibradores e predições\n",
    "iso_calibrators = []\n",
    "platt_calibrators = []\n",
    "test_predictions_raw = []\n",
    "oof_predictions = np.zeros(len(X))  # Out-of-fold predictions\n",
    "\n",
    "print(\"🔄 Treinando modelos e calibradores...\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Processando Fold {fold}/5...\")\n",
    "    \n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    X_val_fold = X.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    # Treinar modelo\n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # Predições no validation set (para treinar calibrador)\n",
    "    y_val_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    oof_predictions[val_idx] = y_val_pred\n",
    "    \n",
    "    # Treinar calibradores\n",
    "    # Isotonic Regression\n",
    "    iso_cal = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso_cal.fit(y_val_pred, y_val_fold)\n",
    "    iso_calibrators.append(iso_cal)\n",
    "    \n",
    "    # Platt Scaling\n",
    "    platt_cal = LogisticRegression()\n",
    "    platt_cal.fit(y_val_pred.reshape(-1, 1), y_val_fold)\n",
    "    platt_calibrators.append(platt_cal)\n",
    "    \n",
    "    # Predições no test set\n",
    "    y_test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    test_predictions_raw.append(y_test_pred)\n",
    "    \n",
    "    print(f\"  ✓ Fold {fold} completo\\n\")\n",
    "\n",
    "# Média das predições no test set (sem calibrar)\n",
    "test_pred_raw_mean = np.mean(test_predictions_raw, axis=0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"APLICANDO CALIBRAÇÃO NAS PREDIÇÕES DO TEST SET\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Aplicar cada calibrador e fazer média\n",
    "test_predictions_iso = []\n",
    "test_predictions_platt = []\n",
    "\n",
    "for i, (iso_cal, platt_cal) in enumerate(zip(iso_calibrators, platt_calibrators)):\n",
    "    # Isotonic\n",
    "    pred_iso = iso_cal.predict(test_predictions_raw[i])\n",
    "    test_predictions_iso.append(pred_iso)\n",
    "    \n",
    "    # Platt\n",
    "    pred_platt = platt_cal.predict_proba(test_predictions_raw[i].reshape(-1, 1))[:, 1]\n",
    "    test_predictions_platt.append(pred_platt)\n",
    "\n",
    "# Média das predições calibradas\n",
    "test_pred_iso_mean = np.mean(test_predictions_iso, axis=0)\n",
    "test_pred_platt_mean = np.mean(test_predictions_platt, axis=0)\n",
    "\n",
    "# Comparar estatísticas\n",
    "print(\"📊 ESTATÍSTICAS DAS PREDIÇÕES:\\n\")\n",
    "print(f\"{'Método':<25} {'Média':<12} {'Mediana':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\" * 73)\n",
    "print(f\"{'Sem Calibração':<25} {test_pred_raw_mean.mean():<12.5f} {np.median(test_pred_raw_mean):<12.5f} {test_pred_raw_mean.min():<12.5f} {test_pred_raw_mean.max():<12.5f}\")\n",
    "print(f\"{'Isotonic Regression':<25} {test_pred_iso_mean.mean():<12.5f} {np.median(test_pred_iso_mean):<12.5f} {test_pred_iso_mean.min():<12.5f} {test_pred_iso_mean.max():<12.5f}\")\n",
    "print(f\"{'Platt Scaling':<25} {test_pred_platt_mean.mean():<12.5f} {np.median(test_pred_platt_mean):<12.5f} {test_pred_platt_mean.min():<12.5f} {test_pred_platt_mean.max():<12.5f}\")\n",
    "\n",
    "# Verificar taxa esperada no test\n",
    "print(f\"\\n💡 Taxa de conversão esperada no train: {y.mean():.5f} ({y.mean()*100:.2f}%)\")\n",
    "print(f\"   Predição média calibrada (Isotonic): {test_pred_iso_mean.mean():.5f} ({test_pred_iso_mean.mean()*100:.2f}%)\")\n",
    "print(f\"   Diferença: {abs(y.mean() - test_pred_iso_mean.mean()):.5f}\\n\")\n",
    "\n",
    "# Salvar submissões calibradas\n",
    "test_ids = test['id'].values if 'id' in test.columns else range(len(test))\n",
    "\n",
    "# Submissão 1: Isotonic Regression (recomendada)\n",
    "submission_iso = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'y': test_pred_iso_mean\n",
    "})\n",
    "submission_iso.to_csv('../submissions/submission_lgb_calibrated_isotonic.csv', index=False)\n",
    "print(\"✅ Salvo: submission_lgb_calibrated_isotonic.csv\")\n",
    "\n",
    "# Submissão 2: Platt Scaling\n",
    "submission_platt = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'y': test_pred_platt_mean\n",
    "})\n",
    "submission_platt.to_csv('../submissions/submission_lgb_calibrated_platt.csv', index=False)\n",
    "print(\"✅ Salvo: submission_lgb_calibrated_platt.csv\")\n",
    "\n",
    "# Submissão 3: Top 30 features calibradas\n",
    "print(\"\\n🔄 Gerando versão calibrada do Top 30 features...\")\n",
    "\n",
    "# Top 30 features\n",
    "feature_importance = pd.read_csv('../data/processed/feature_importance.csv')\n",
    "top_30_features = feature_importance.head(30)['feature'].tolist()\n",
    "\n",
    "X_top30 = X[top_30_features]\n",
    "X_test_top30 = X_test[top_30_features]\n",
    "\n",
    "# Repetir processo para top 30\n",
    "iso_calibrators_top30 = []\n",
    "test_predictions_top30 = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_top30, y), 1):\n",
    "    X_train_fold = X_top30.iloc[train_idx]\n",
    "    X_val_fold = X_top30.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_val_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    \n",
    "    iso_cal = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso_cal.fit(y_val_pred, y_val_fold)\n",
    "    iso_calibrators_top30.append(iso_cal)\n",
    "    \n",
    "    y_test_pred = model.predict(X_test_top30, num_iteration=model.best_iteration)\n",
    "    test_predictions_top30.append(y_test_pred)\n",
    "\n",
    "# Calibrar predições top30\n",
    "test_predictions_top30_calibrated = []\n",
    "for i, iso_cal in enumerate(iso_calibrators_top30):\n",
    "    pred_cal = iso_cal.predict(test_predictions_top30[i])\n",
    "    test_predictions_top30_calibrated.append(pred_cal)\n",
    "\n",
    "test_pred_top30_cal_mean = np.mean(test_predictions_top30_calibrated, axis=0)\n",
    "\n",
    "submission_top30_cal = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'y': test_pred_top30_cal_mean\n",
    "})\n",
    "submission_top30_cal.to_csv('../submissions/submission_top30_calibrated.csv', index=False)\n",
    "print(\"✅ Salvo: submission_top30_calibrated.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMO DAS SUBMISSÕES DISPONÍVEIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"📁 SUBMISSÕES SEM CALIBRAÇÃO (geradas anteriormente):\")\n",
    "print(\"   1. submission_lgb_top30.csv\")\n",
    "print(\"   2. submission_lgb_tuned.csv\")\n",
    "print(\"   3. submission_ensemble_optimized.csv\")\n",
    "\n",
    "print(\"\\n📁 SUBMISSÕES CALIBRADAS (NOVAS - RECOMENDADAS):\")\n",
    "print(\"   4. submission_lgb_calibrated_isotonic.csv    ⭐ MELHOR OPÇÃO\")\n",
    "print(\"   5. submission_lgb_calibrated_platt.csv\")\n",
    "print(\"   6. submission_top30_calibrated.csv           ⭐ ALTERNATIVA ROBUSTA\")\n",
    "\n",
    "print(\"\\n💡 RECOMENDAÇÃO FINAL:\")\n",
    "print(\"   Submeta PRIMEIRO: submission_lgb_calibrated_isotonic.csv\")\n",
    "print(\"   Segunda opção:    submission_top30_calibrated.csv\")\n",
    "print(\"   Terceira opção:   submission_ensemble_optimized.csv (já gerado)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb94e5",
   "metadata": {},
   "source": [
    "# tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5110624",
   "metadata": {},
   "source": [
    "## Calibração de Probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d621d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ANÁLISE DE CALIBRAÇÃO DO MODELO (CORRIGIDO)\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
    "\n",
    "# Carregar dados\n",
    "train = pd.read_csv('../data/processed/train_processed.csv')\n",
    "y = train['y']\n",
    "X = train.drop(['y', 'id'], axis=1, errors='ignore')\n",
    "\n",
    "# Pegar predições de um dos folds para análise\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = list(skf.split(X, y))[0]\n",
    "\n",
    "X_train_fold = X.iloc[train_idx]\n",
    "X_val = X.iloc[val_idx]\n",
    "y_train_fold = y.iloc[train_idx]\n",
    "y_val = y.iloc[val_idx]\n",
    "\n",
    "# Retreinar um modelo rápido para análise\n",
    "train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "\n",
    "best_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 88,\n",
    "    'learning_rate': 0.068,\n",
    "    'feature_fraction': 0.60,\n",
    "    'bagging_fraction': 0.96,\n",
    "    'bagging_freq': 1,\n",
    "    'min_child_samples': 48,\n",
    "    'max_depth': 12,\n",
    "    'scale_pos_weight': (y == 0).sum() / (y == 1).sum(),\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(best_params, train_data, num_boost_round=800)\n",
    "\n",
    "# Predições\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANÁLISE DE CALIBRAÇÃO DO MODELO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Comparar taxa real vs predita\n",
    "print(f\"\\n📊 COMPARAÇÃO DE TAXAS:\")\n",
    "print(f\"   Taxa real (y_val):        {y_val.mean():.4f} ({y_val.mean()*100:.2f}%)\")\n",
    "print(f\"   Predição média (modelo):  {y_pred.mean():.4f} ({y_pred.mean()*100:.2f}%)\")\n",
    "print(f\"   Diferença absoluta:       {abs(y_val.mean() - y_pred.mean()):.4f}\")\n",
    "print(f\"   Diferença relativa:       {((y_pred.mean() / y_val.mean()) - 1)*100:.1f}%\")\n",
    "\n",
    "# 2. Análise por bins de probabilidade (CORRIGIDO)\n",
    "print(f\"\\n📈 ANÁLISE POR BINS DE PROBABILIDADE:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Bin de Predição':<20} {'Taxa Real':<15} {'Pred Média':<15} {'Quantidade':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "y_val_binned = pd.cut(y_pred, bins=bins)\n",
    "\n",
    "# Corrigido: usar .categories diretamente\n",
    "for bin_range in y_val_binned.categories:\n",
    "    mask = y_val_binned == bin_range\n",
    "    count = mask.sum()\n",
    "    if count > 0:\n",
    "        real_rate = y_val[mask].mean()\n",
    "        pred_mean = y_pred[mask].mean()\n",
    "        print(f\"{str(bin_range):<20} {real_rate:<15.3f} {pred_mean:<15.3f} {count:<15,}\")\n",
    "\n",
    "# 3. Brier Score (mede calibração)\n",
    "brier = brier_score_loss(y_val, y_pred)\n",
    "print(f\"\\n📉 BRIER SCORE: {brier:.5f}\")\n",
    "print(\"   (Quanto menor, melhor. 0 = perfeito, 0.25 = aleatório)\")\n",
    "\n",
    "# Referência: Brier Score de um modelo aleatório\n",
    "brier_random = y_val.mean() * (1 - y_val.mean())\n",
    "print(f\"   Brier Score (modelo aleatório): {brier_random:.5f}\")\n",
    "print(f\"   Melhoria sobre aleatório: {((brier_random - brier) / brier_random * 100):.1f}%\")\n",
    "\n",
    "# 4. AUC-ROC\n",
    "auc = roc_auc_score(y_val, y_pred)\n",
    "print(f\"\\n📈 AUC-ROC: {auc:.5f}\")\n",
    "\n",
    "# 5. Análise específica: quantos % dos casos têm pred > 0.5?\n",
    "print(f\"\\n🎯 PREDIÇÕES COM ALTA CONFIANÇA:\")\n",
    "\n",
    "for threshold in [0.3, 0.5, 0.7, 0.9]:\n",
    "    high_prob = (y_pred > threshold).sum()\n",
    "    high_prob_pct = high_prob / len(y_pred) * 100\n",
    "    if high_prob > 0:\n",
    "        real_rate = y_val[y_pred > threshold].mean()\n",
    "        print(f\"   >={threshold:.1f}: {high_prob:,} casos ({high_prob_pct:.2f}%) | Taxa real: {real_rate:.3f}\")\n",
    "\n",
    "# 6. Curva de calibração\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_val, y_pred, n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Curva de calibração\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--', label='Perfeitamente calibrado', linewidth=2)\n",
    "axes[0, 0].plot(mean_predicted_value, fraction_of_positives, 's-', \n",
    "                label='LightGBM Tuned', linewidth=2, markersize=8, color='red')\n",
    "axes[0, 0].set_xlabel('Probabilidade Predita Média', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Fração de Positivos Real', fontweight='bold')\n",
    "axes[0, 0].set_title('Curva de Calibração', fontweight='bold', fontsize=14)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "axes[0, 0].set_xlim([0, 1])\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "\n",
    "# Plot 2: Distribuição das predições vs real\n",
    "axes[0, 1].hist(y_pred[y_val == 0], bins=50, alpha=0.5, label='y=0 (não aderiu)', \n",
    "                density=True, color='red')\n",
    "axes[0, 1].hist(y_pred[y_val == 1], bins=50, alpha=0.5, label='y=1 (aderiu)', \n",
    "                density=True, color='green')\n",
    "axes[0, 1].axvline(y_val.mean(), color='blue', linestyle='--', linewidth=2,\n",
    "                   label=f'Taxa real: {y_val.mean():.3f}')\n",
    "axes[0, 1].axvline(y_pred.mean(), color='orange', linestyle='--', linewidth=2,\n",
    "                   label=f'Pred média: {y_pred.mean():.3f}')\n",
    "axes[0, 1].set_xlabel('Probabilidade', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Densidade', fontweight='bold')\n",
    "axes[0, 1].set_title('Distribuição das Predições', fontweight='bold', fontsize=14)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Reliability diagram (outra forma de visualizar calibração)\n",
    "axes[1, 0].bar(mean_predicted_value, fraction_of_positives - mean_predicted_value,\n",
    "               width=0.08, alpha=0.7, color='coral')\n",
    "axes[1, 0].axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1, 0].set_xlabel('Probabilidade Predita Média', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Diferença (Real - Predita)', fontweight='bold')\n",
    "axes[1, 0].set_title('Reliability Diagram (Erro de Calibração)', fontweight='bold', fontsize=14)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Distribuição acumulada\n",
    "sorted_pred = np.sort(y_pred)\n",
    "cumsum = np.arange(1, len(sorted_pred) + 1) / len(sorted_pred)\n",
    "axes[1, 1].plot(sorted_pred, cumsum, linewidth=2, color='purple')\n",
    "axes[1, 1].axhline(0.5, color='red', linestyle='--', label='Mediana')\n",
    "axes[1, 1].axvline(y_val.mean(), color='blue', linestyle='--', label=f'Taxa real: {y_val.mean():.3f}')\n",
    "axes[1, 1].set_xlabel('Probabilidade Predita', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Proporção Acumulada', fontweight='bold')\n",
    "axes[1, 1].set_title('Distribuição Acumulada das Predições', fontweight='bold', fontsize=14)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74d7e9",
   "metadata": {},
   "source": [
    "## Se a Calibração Estiver Ruim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c28e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CALIBRAÇÃO DE PROBABILIDADES (SE NECESSÁRIO)\n",
    "# ==============================================================================\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APLICANDO CALIBRAÇÃO NAS PROBABILIDADES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Método 1: Isotonic Regression (mais flexível)\n",
    "iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "iso_reg.fit(y_pred, y_val)\n",
    "y_pred_calibrated_iso = iso_reg.predict(y_pred)\n",
    "\n",
    "print(f\"📊 RESULTADOS DA CALIBRAÇÃO (Isotonic):\")\n",
    "print(f\"   Antes: {y_pred.mean():.4f}\")\n",
    "print(f\"   Depois: {y_pred_calibrated_iso.mean():.4f}\")\n",
    "print(f\"   Taxa real: {y_val.mean():.4f}\")\n",
    "print(f\"   Brier Score antes: {brier_score_loss(y_val, y_pred):.5f}\")\n",
    "print(f\"   Brier Score depois: {brier_score_loss(y_val, y_pred_calibrated_iso):.5f}\")\n",
    "\n",
    "# Verificar se AUC mudou (não deveria mudar muito)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_before = roc_auc_score(y_val, y_pred)\n",
    "auc_after = roc_auc_score(y_val, y_pred_calibrated_iso)\n",
    "\n",
    "print(f\"\\n📈 IMPACTO NO AUC:\")\n",
    "print(f\"   AUC antes: {auc_before:.5f}\")\n",
    "print(f\"   AUC depois: {auc_after:.5f}\")\n",
    "print(f\"   Diferença: {auc_after - auc_before:.5f}\")\n",
    "\n",
    "# Método 2: Platt Scaling (regressão logística)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "platt = LogisticRegression()\n",
    "platt.fit(y_pred.reshape(-1, 1), y_val)\n",
    "y_pred_calibrated_platt = platt.predict_proba(y_pred.reshape(-1, 1))[:, 1]\n",
    "\n",
    "print(f\"\\n📊 RESULTADOS DA CALIBRAÇÃO (Platt Scaling):\")\n",
    "print(f\"   Depois (Platt): {y_pred_calibrated_platt.mean():.4f}\")\n",
    "print(f\"   Brier Score (Platt): {brier_score_loss(y_val, y_pred_calibrated_platt):.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
